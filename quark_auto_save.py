#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Modify: 2024-11-13
# Repo: https://github.com/Cp0204/quark_auto_save
# ConfigFile: quark_config.json
"""
new Env('å¤¸å…‹è‡ªåŠ¨è¿½æ›´');
0 8,18,20 * * * quark_auto_save.py
"""
import os
import re
import sys
import json
import time
import random
import requests
import importlib
import traceback
import urllib.parse
from datetime import datetime

# å…¼å®¹é’é¾™
try:
    from treelib import Tree
except:
    print("æ­£åœ¨å°è¯•è‡ªåŠ¨å®‰è£…ä¾èµ–...")
    os.system("pip3 install treelib &> /dev/null")
    from treelib import Tree

# ã€æ–°å¢ã€‘å¼•å…¥ natsort ç”¨äºæ›´æ™ºèƒ½çš„è‡ªç„¶æ’åº
try:
    from natsort import natsorted
except:
    print("æ­£åœ¨å°è¯•è‡ªåŠ¨å®‰è£…ä¾èµ– natsort...")
    os.system("pip3 install natsort &> /dev/null")
    from natsort import natsorted


CONFIG_DATA = {}
NOTIFYS = []
GH_PROXY = os.environ.get("GH_PROXY", "https://ghproxy.net/")

# ã€ä¿ç•™ã€‘ä½ çš„è‡ªå®šä¹‰APIå¼‚å¸¸ï¼Œç”¨äºæ›´ç²¾ç¡®çš„é”™è¯¯æ•è·
class QuarkAPIError(Exception):
    pass

# ã€ä¿ç•™ã€‘ä½ çš„å¯å¤ç”¨çš„æ­¥éª¤é‡è¯•å‡½æ•°
def execute_with_retry(action, description, retries=3, delay=2):
    """
    æ‰§è¡Œä¸€ä¸ªæ“ä½œï¼Œå¹¶åœ¨å¤±è´¥æ—¶é‡è¯•ã€‚
    :param action: ä¸€ä¸ªæ— å‚æ•°çš„å‡½æ•°æˆ–lambdaè¡¨è¾¾å¼ï¼Œä»£è¡¨è¦æ‰§è¡Œçš„æ“ä½œã€‚
    :param description: æ“ä½œçš„æè¿°ï¼Œç”¨äºæ‰“å°æ—¥å¿—ã€‚
    :param retries: æœ€å¤§å°è¯•æ¬¡æ•°ã€‚
    :param delay: æ¯æ¬¡å°è¯•ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ã€‚
    :return: æ“ä½œçš„è¿”å›å€¼ã€‚
    """
    last_exception = None
    for attempt in range(retries):
        try:
            print(f"   ğŸ”„ [å°è¯•æ‰§è¡Œ] {description} (ç¬¬ {attempt + 1}/{retries} æ¬¡å°è¯•)")
            return action()
        except Exception as e:
            last_exception = e
            print(f"   âš ï¸ [æ­¥éª¤å¤±è´¥] {description} (ç¬¬ {attempt + 1}/{retries} æ¬¡å°è¯•): {e}")
            print(f"   ğŸ“‹ [è¯¦ç»†é”™è¯¯] {type(e).__name__}: {str(e)}")
            if attempt < retries - 1:
                print(f"   â±ï¸ [ç­‰å¾…é‡è¯•] ç­‰å¾… {delay} ç§’åé‡è¯•...")
                time.sleep(delay)
    print(f"   âŒ [å½»åº•å¤±è´¥] {description} åœ¨ {retries} æ¬¡å°è¯•åå‡å‘Šå¤±è´¥ã€‚")
    print(f"   ğŸ“‹ [æœ€ç»ˆé”™è¯¯] {type(last_exception).__name__}: {str(last_exception)}")
    raise last_exception


# å‘é€é€šçŸ¥æ¶ˆæ¯
def send_ql_notify(title, body):
    try:
        import notify
        if CONFIG_DATA.get("push_config"):
            notify.push_config.update(CONFIG_DATA["push_config"])
            notify.push_config["CONSOLE"] = notify.push_config.get("CONSOLE", True)
        notify.send(title, body)
    except Exception as e:
        if e:
            print("å‘é€é€šçŸ¥æ¶ˆæ¯å¤±è´¥ï¼")


# æ·»åŠ æ¶ˆæ¯
def add_notify(text):
    global NOTIFYS
    NOTIFYS.append(text)
    print("ğŸ“¢", text)
    return text

# ã€æ–°å¢ã€‘æ ¼å¼åŒ–å­—èŠ‚å¤§å°çš„è¾…åŠ©å‡½æ•°ï¼Œç”¨äºç­¾åˆ°åŠŸèƒ½
def format_bytes(size_bytes: int) -> str:
    if not isinstance(size_bytes, (int, float)) or size_bytes < 0:
        return "0 B"
    units = ("B", "KB", "MB", "GB", "TB", "PB")
    i = 0
    while size_bytes >= 1024 and i < len(units) - 1:
        size_bytes /= 1024
        i += 1
    return f"{size_bytes:.2f} {units[i]}"


class Config:
    def download_file(url, save_path):
        response = requests.get(url)
        if response.status_code == 200:
            with open(save_path, "wb") as file:
                file.write(response.content)
            return True
        else:
            return False

    def read_json(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data

    def write_json(config_path, data):
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, sort_keys=False, indent=2)

    def get_cookies(cookie_val):
        if isinstance(cookie_val, list):
            return cookie_val
        elif cookie_val:
            return cookie_val.split("\n") if "\n" in cookie_val else [cookie_val]
        else:
            return False

    def load_plugins(plugins_config={}, plugins_dir="plugins"):
        PLUGIN_FLAGS = os.environ.get("PLUGIN_FLAGS", "").split(",")
        plugins_available = {}
        task_plugins_config = {}
        all_modules = [
            f.replace(".py", "") for f in os.listdir(plugins_dir) if f.endswith(".py")
        ]
        priority_path = os.path.join(plugins_dir, "_priority.json")
        try:
            with open(priority_path, encoding="utf-8") as f:
                priority_modules = json.load(f)
            if priority_modules:
                all_modules = [
                    module for module in priority_modules if module in all_modules
                ] + [module for module in all_modules if module not in priority_modules]
        except (FileNotFoundError, json.JSONDecodeError):
            pass
        for module_name in all_modules:
            if f"-{module_name}" in PLUGIN_FLAGS:
                continue
            try:
                module = importlib.import_module(f"{plugins_dir}.{module_name}")
                ServerClass = getattr(module, module_name.capitalize())
                if module_name in plugins_config:
                    plugin = ServerClass(**plugins_config[module_name])
                    plugins_available[module_name] = plugin
                else:
                    plugin = ServerClass()
                    plugins_config[module_name] = plugin.default_config
                if hasattr(plugin, "default_task_config"):
                    task_plugins_config[module_name] = plugin.default_task_config
            except (ImportError, AttributeError) as e:
                print(f"è½½å…¥æ¨¡å— {module_name} å¤±è´¥: {e}")
        print()
        return plugins_available, plugins_config, task_plugins_config

    def breaking_change_update(config_data):
        for task in config_data.get("tasklist", []):
            if "$TASKNAME" in task.get("replace", ""):
                task["replace"] = task["replace"].replace("$TASKNAME", "{TASKNAME}")


class MagicRename:
    magic_regex = {
        "$TV": {"pattern": r".*?([Ss]\d{1,2})?(?:[ç¬¬EePpXx\.\-\_\( ]{1,2}|^)(\d{1,3})(?!\d).*?\.(mp4|mkv)", "replace": r"\1E\2.\3"},
        "$BLACK_WORD": {"pattern": r"^(?!.*çº¯äº«)(?!.*åŠ æ›´)(?!.*è¶…å‰ä¼åˆ’)(?!.*è®­ç»ƒå®¤)(?!.*è’¸è’¸æ—¥ä¸Š).*", "replace": ""},
    }
    magic_variable = {
        "{TASKNAME}": "", "{I}": 1, "{EXT}": [r"(?<=\.)\w+$"], "{CHINESE}": [r"[\u4e00-\u9fa5]{2,}"],
        "{DATE}": [r"(18|19|20)?\d{2}[\.\-/å¹´]\d{1,2}[\.\-/æœˆ]\d{1,2}", r"(?<!\d)[12]\d{3}[01]?\d[0123]?\d", r"(?<!\d)[01]?\d[\.\-/æœˆ][0123]?\d"],
        "{YEAR}": [r"(?<!\d)(18|19|20)\d{2}(?!\d)"], "{S}": [r"(?<=[Ss])\d{1,2}(?=[EeXx])", r"(?<=[Ss])\d{1,2}"],
        "{SXX}": [r"[Ss]\d{1,2}(?=[EeXx])", r"[Ss]\d{1,2}"],
        "{E}": [r"(?<=[Ss]\d\d[Ee])\d{1,3}", r"(?<=[Ee])\d{1,3}", r"(?<=[Ee][Pp])\d{1,3}", r"(?<=ç¬¬)\d{1,3}(?=[é›†æœŸè¯éƒ¨ç¯‡])", r"(?<!\d)\d{1,3}(?=[é›†æœŸè¯éƒ¨ç¯‡])", r"(?!.*19)(?!.*20)(?<=[\._])\d{1,3}(?=[\._])", r"^\d{1,3}(?=\.\w+)", r"(?<!\d)\d{1,3}(?!\d)(?!$)"],
        "{PART}": [r"(?<=[é›†æœŸè¯éƒ¨ç¯‡ç¬¬])[ä¸Šä¸­ä¸‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]", r"[ä¸Šä¸­ä¸‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]"], "{VER}": [r"[\u4e00-\u9fa5]+ç‰ˆ"],
    }
    # ã€ä¼˜åŒ–ã€‘æ‰©å±•ä¸­æ–‡æ•°å­—å¹¶ä¸ºæ’åºé”®æ·»åŠ åˆ†éš”ç¬¦ï¼Œé¿å…å½±å“ç›¸é‚»æ•°å­—
    priority_list = ["ä¸Š", "ä¸­", "ä¸‹", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å", "ç™¾", "åƒ", "ä¸‡"]

    def __init__(self, magic_regex={}, magic_variable={}):
        self.magic_regex.update(magic_regex)
        self.magic_variable.update(magic_variable)
        self.dir_filename_dict = {}

    def set_taskname(self, taskname):
        self.magic_variable["{TASKNAME}"] = taskname

    def magic_regex_conv(self, pattern, replace):
        keyword = pattern
        if keyword in self.magic_regex:
            pattern = self.magic_regex[keyword]["pattern"]
            if replace == "":
                replace = self.magic_regex[keyword]["replace"]
        return pattern, replace

    def sub(self, pattern, replace, file_name):
        if not replace: return file_name
        for key, p_list in self.magic_variable.items():
            if key in replace:
                if p_list and isinstance(p_list, list):
                    match = None
                    for p in p_list:
                        match = re.search(p, file_name)
                        if match:
                            value = match.group()
                            if key == "{DATE}":
                                value = "".join([char for char in value if char.isdigit()])
                                value = str(datetime.now().year)[: (8 - len(value))] + value
                            replace = replace.replace(key, value)
                            break
                if key == "{TASKNAME}": replace = replace.replace(key, self.magic_variable["{TASKNAME}"])
                elif key == "{SXX}" and not match: replace = replace.replace(key, "S01")
                elif key == "{I}": continue
                else: replace = replace.replace(key, "")
        return re.sub(pattern, replace, file_name) if pattern and replace else replace

    # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨æ›´å®‰å…¨çš„æ›¿æ¢æ–¹å¼ï¼Œé˜²æ­¢å…³é”®å­—è¯¯åŒ¹é…
    def _custom_sort_key(self, name):
        for i, keyword in enumerate(self.priority_list):
            if keyword in name:
                name = name.replace(keyword, f"_{i:02d}_")
        return name

    # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨ natsorted è¿›è¡Œè‡ªç„¶æ’åºï¼Œå¹¶åŠ å…¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä½œä¸ºæ’åºå› ç´ 
    def sort_file_list(self, file_list, dir_filename_dict={}):
        filename_list = [
            f"{f['file_name_re']}_{f['updated_at']}"
            for f in file_list
            if f.get("file_name_re") and not f["dir"]
        ]
        dir_filename_dict = dir_filename_dict or self.dir_filename_dict
        filename_list = list(set(filename_list) | set(dir_filename_dict.values()))
        # ä½¿ç”¨ natsorted è¿›è¡Œè‡ªç„¶æ’åº
        filename_list = natsorted(filename_list, key=self._custom_sort_key)
        
        filename_index = {}
        for name in filename_list:
            if name in dir_filename_dict.values(): continue
            i = filename_list.index(name) + 1
            while i in dir_filename_dict.keys(): i += 1
            dir_filename_dict[i] = name
            filename_index[name] = i
        
        for file in file_list:
            if file.get("file_name_re"):
                if match := re.search(r"\{I+\}", file["file_name_re"]):
                    # ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åæ¥æŸ¥æ‰¾ç´¢å¼•
                    i = filename_index.get(f"{file['file_name_re']}_{file['updated_at']}", 0)
                    file["file_name_re"] = re.sub(match.group(), str(i).zfill(match.group().count("I")), file["file_name_re"])

    def set_dir_file_list(self, file_list, replace):
        if not file_list: return
        self.dir_filename_dict = {}
        filename_list = [f["file_name"] for f in file_list if not f["dir"]]
        filename_list.sort()
        if match := re.search(r"\{I+\}", replace):
            magic_i = match.group()
            pattern_i = r"\d" * magic_i.count("I")
            pattern = replace.replace(match.group(), "ğŸ”¢")
            for key, _ in self.magic_variable.items():
                if key in pattern: pattern = pattern.replace(key, "ğŸ”£")
            pattern = re.sub(r"\\[0-9]+", "ğŸ”£", pattern)
            pattern = f"({re.escape(pattern).replace('ğŸ”£', '.*?').replace('ğŸ”¢', f')({pattern_i})(')})"
            if filename_list and (match_last := re.match(pattern, filename_list[-1])):
                self.magic_variable["{I}"] = int(match_last.group(2))
            for filename in filename_list:
                if match_item := re.match(pattern, filename):
                    self.dir_filename_dict[int(match_item.group(2))] = (match_item.group(1) + magic_i + match_item.group(3))

    def is_exists(self, filename, filename_list, ignore_ext=False):
        if not filename: return None
        if ignore_ext:
            filename_no_ext = os.path.splitext(filename)[0]
            filename_list_no_ext = [os.path.splitext(f)[0] for f in filename_list]
            for i, f in enumerate(filename_list_no_ext):
                if f == filename_no_ext: return filename_list[i]
            return None
        else:
            if match := re.search(r"\{I+\}", filename):
                magic_i = match.group()
                pattern_i = r"\d" * magic_i.count("I")
                pattern = re.escape(filename).replace(f"\\{magic_i}", pattern_i)
                for f in filename_list:
                    if re.fullmatch(pattern, f): return f
                return None
            else:
                return filename if filename in filename_list else None


class Quark:
    BASE_URL = "https://drive-pc.quark.cn"
    BASE_URL_APP = "https://drive-m.quark.cn"
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) quark-cloud-drive/3.14.2 Chrome/112.0.5615.165 Electron/24.1.3.8 Safari/537.36 Channel/pckk_other_ch"
    
    def __init__(self, cookie="", index=0, blacklist=[]):
        self.cookie = cookie.strip()
        self.index = index + 1
        self.is_active = False
        self.nickname = ""
        self.mparam = self._match_mparam_form_cookie(cookie)
        self.file_blacklist = blacklist
        self.savepath_fid = {"/": "0"}

    def _match_mparam_form_cookie(self, cookie):
        mparam = {}
        kps_match = re.search(r"(?<!\w)kps=([a-zA-Z0-9%+/=]+)[;&]?", cookie)
        sign_match = re.search(r"(?<!\w)sign=([a-zA-Z0-9%+/=]+)[;&]?", cookie)
        vcode_match = re.search(r"(?<!\w)vcode=([a-zA-Z0-9%+/=]+)[;&]?", cookie)
        if kps_match and sign_match and vcode_match:
            return {"kps": kps_match.group(1).replace("%25", "%"), "sign": sign_match.group(1).replace("%25", "%"), "vcode": vcode_match.group(1).replace("%25", "%")}
        return mparam

    def _send_request(self, method, url, **kwargs):
        headers = {"cookie": self.cookie, "content-type": "application/json", "user-agent": self.USER_AGENT}
        if "headers" in kwargs: headers.update(kwargs.pop("headers"))
        
        # è®¾ç½®é»˜è®¤è¶…æ—¶æ—¶é—´
        kwargs.setdefault('timeout', (5, 30)) # (è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
        
        # å¤„ç†ç§»åŠ¨ç«¯å‚æ•°
        if self.mparam and "share" in url and self.BASE_URL in url:
            url = url.replace(self.BASE_URL, self.BASE_URL_APP)
            kwargs.setdefault("params", {}).update({
                "device_model": "M2011K2C", "entry": "default_clouddrive", "_t_group": "0%3A_s_vp%3A1", "dmn": "Mi%2B11", "fr": "android",
                "pf": "3300", "bi": "35937", "ve": "7.4.5.680", "ss": "411x875", "mi": "M2011K2C", "nt": "5", "nw": "0", "kt": "4",
                "pr": "ucpro", "sv": "release", "dt": "phone", "data_from": "ucapi", **self.mparam, "app": "clouddrive", "kkkk": "1",
            })
            if "cookie" in headers: del headers["cookie"]
        
        try:
            print(f"   ğŸ“¡ [å‘é€è¯·æ±‚] {method} {url}")
            response = requests.request(method, url, headers=headers, **kwargs)
            if response.status_code != 200:
                print(f"   âŒ [HTTPé”™è¯¯] çŠ¶æ€ç : {response.status_code}, URL: {url}")
                response.raise_for_status()
            return response
        except requests.exceptions.Timeout:
            print(f"   â±ï¸ [è¯·æ±‚è¶…æ—¶] URL: {url}, è¶…æ—¶æ—¶é—´: {kwargs.get('timeout')}ç§’")
            raise QuarkAPIError(f"è¯·æ±‚è¶…æ—¶: {url}")
        except requests.exceptions.RequestException as e:
            print(f"   âŒ [è¯·æ±‚å¼‚å¸¸] URL: {url}, é”™è¯¯: {type(e).__name__}: {str(e)}")
            raise QuarkAPIError(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")
        except Exception as e:
            print(f"   âŒ [æœªçŸ¥é”™è¯¯] URL: {url}, é”™è¯¯: {type(e).__name__}: {str(e)}")
            raise

    def init(self):
        account_info = self.get_account_info()
        self.is_active = True
        self.nickname = account_info["nickname"]

    def get_account_info(self):
        response = self._send_request("GET", "https://pan.quark.cn/account/info", params={"fr": "pc", "platform": "pc"}).json()
        if not response.get("data"): raise QuarkAPIError(f"è·å–è´¦æˆ·ä¿¡æ¯å¤±è´¥: {response.get('message')}")
        return response["data"]

    # ã€æ–°å¢ã€‘è·å–ç­¾åˆ°ä¿¡æ¯çš„API
    def get_growth_info(self):
        if not self.mparam: return None
        params = {"pr": "ucpro", "fr": "android", **self.mparam}
        headers = {"content-type": "application/json"}
        response = self._send_request("GET", f"{self.BASE_URL_APP}/1/clouddrive/capacity/growth/info", params=params, headers=headers).json()
        if response.get("data"): return response["data"]
        return None

    # ã€æ–°å¢ã€‘æ‰§è¡Œç­¾åˆ°çš„API
    def get_growth_sign(self):
        if not self.mparam: return False, "ç§»åŠ¨ç«¯å‚æ•°æœªé…ç½®"
        params = {"pr": "ucpro", "fr": "android", **self.mparam}
        payload = {"sign_cyclic": True}
        headers = {"content-type": "application/json"}
        response = self._send_request("POST", f"{self.BASE_URL_APP}/1/clouddrive/capacity/growth/sign", json=payload, params=params, headers=headers).json()
        if response.get("data"):
            return True, response["data"]["sign_daily_reward"]
        return False, response.get("message", "ç­¾åˆ°å¤±è´¥")

    def get_stoken(self, pwd_id, passcode=""):
        response = self._send_request("POST", f"{self.BASE_URL}/1/clouddrive/share/sharepage/token", json={"pwd_id": pwd_id, "passcode": passcode}, params={"pr": "ucpro", "fr": "pc"}).json()
        if response.get("status") != 200: raise QuarkAPIError(response.get('message', 'è·å–stokenå¤±è´¥'))
        return response["data"]["stoken"]

    def get_detail(self, pwd_id, stoken, pdir_fid):
        list_merge, page = [], 1
        print(f"   ğŸ“¡ [å¼€å§‹è¯·æ±‚] è·å–åˆ†äº«å†…å®¹ï¼Œpwd_id={pwd_id}, pdir_fid={pdir_fid}")
        while True:
            try:
                params = {"pr": "ucpro", "fr": "pc", "pwd_id": pwd_id, "stoken": stoken, "pdir_fid": pdir_fid, "force": "0", "_page": page, "_size": "50", "_fetch_banner": "0", "_fetch_share": 0, "_fetch_total": "1", "_sort": "file_type:asc,updated_at:desc"}
                print(f"   ğŸ“¡ [è¯·æ±‚å‚æ•°] ç¬¬{page}é¡µ: {params}")
                response = self._send_request("GET", f"{self.BASE_URL}/1/clouddrive/share/sharepage/detail", params=params, timeout=30).json()
                if response["code"] != 0: 
                    print(f"   âŒ [APIé”™è¯¯] è·å–åˆ†äº«è¯¦æƒ…å¤±è´¥: {response['message']}")
                    raise QuarkAPIError(f"è·å–åˆ†äº«è¯¦æƒ…å¤±è´¥: {response['message']}")
                print(f"   âœ… [è¯·æ±‚æˆåŠŸ] ç¬¬{page}é¡µï¼Œè·å–åˆ° {len(response['data']['list'])} ä¸ªé¡¹ç›®")
                list_merge.extend(response["data"]["list"])
                if not response["data"]["list"] or len(list_merge) >= response["metadata"]["_total"]: 
                    print(f"   âœ… [è¯·æ±‚å®Œæˆ] å…±è·å–åˆ° {len(list_merge)} ä¸ªé¡¹ç›®")
                    break
                page += 1
            except Exception as e:
                print(f"   âŒ [è¯·æ±‚å¼‚å¸¸] è·å–åˆ†äº«å†…å®¹ç¬¬{page}é¡µå¤±è´¥: {type(e).__name__}: {str(e)}")
                raise
        return list_merge

    def ls_dir(self, pdir_fid):
        list_merge, page = [], 1
        while True:
            params = {"pr": "ucpro", "fr": "pc", "pdir_fid": pdir_fid, "_page": page, "_size": "50", "_fetch_total": "1"}
            response = self._send_request("GET", f"{self.BASE_URL}/1/clouddrive/file/sort", params=params).json()
            if response["code"] != 0: raise QuarkAPIError(f"åˆ—å‡ºç›®å½•å¤±è´¥: {response['message']}")
            list_merge.extend(response["data"]["list"])
            if not response["data"]["list"] or len(list_merge) >= response["metadata"]["_total"]: break
            page += 1
        return list_merge

    def save_file(self, fid_list, fid_token_list, to_pdir_fid, pwd_id, stoken):
        params = {"pr": "ucpro", "fr": "pc", "app": "clouddrive", "__t": int(time.time() * 1000)}
        payload = {"fid_list": fid_list, "fid_token_list": fid_token_list, "to_pdir_fid": to_pdir_fid, "pwd_id": pwd_id, "stoken": stoken}
        response = self._send_request("POST", f"{self.BASE_URL}/1/clouddrive/share/sharepage/save", json=payload, params=params).json()
        if response.get("code") != 0: raise QuarkAPIError(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {response.get('message')}")
        return self.query_task(response["data"]["task_id"])

    def query_task(self, task_id):
        description = f"æŸ¥è¯¢ä»»åŠ¡ {task_id}"
        def action():
            params = {"pr": "ucpro", "fr": "pc", "task_id": task_id, "__t": int(time.time() * 1000)}
            response = self._send_request("GET", f"{self.BASE_URL}/1/clouddrive/task", params=params).json()
            if response.get("data", {}).get("status") == 0:
                raise QuarkAPIError(f"ä»»åŠ¡ä»åœ¨æ‰§è¡Œä¸­: {response.get('data', {}).get('task_title')}")
            if response.get("code") != 0:
                raise QuarkAPIError(f"æŸ¥è¯¢ä»»åŠ¡å¤±è´¥: {response.get('message')}")
            return response["data"]
        return execute_with_retry(action, description, retries=10, delay=1)
    
    def mkdir(self, dir_path):
        payload = {"pdir_fid": "0", "file_name": "", "dir_path": dir_path}
        response = self._send_request("POST", f"{self.BASE_URL}/1/clouddrive/file", json=payload, params={"pr": "ucpro", "fr": "pc"}).json()
        if response.get("code") != 0: raise QuarkAPIError(f"åˆ›å»ºç›®å½• '{dir_path}' å¤±è´¥: {response.get('message')}")
        return response["data"]

    def rename(self, fid, file_name):
        payload = {"fid": fid, "file_name": file_name}
        response = self._send_request("POST", f"{self.BASE_URL}/1/clouddrive/file/rename", json=payload, params={"pr": "ucpro", "fr": "pc"}).json()
        if response.get("code") != 0: raise QuarkAPIError(f"é‡å‘½åå¤±è´¥: {response.get('message')}")
        return response
    
    # ã€æ–°å¢ã€‘åˆ é™¤æ–‡ä»¶çš„APIï¼Œç”¨äºå­ç›®å½•é‡å­˜
    def delete(self, filelist):
        payload = {"action_type": 2, "filelist": filelist, "exclude_fids": []}
        response = self._send_request("POST", f"{self.BASE_URL}/1/clouddrive/file/delete", json=payload, params={"pr": "ucpro", "fr": "pc"}).json()
        if response.get("code") != 0: raise QuarkAPIError(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {response.get('message')}")
        return response["data"]

    # ã€æ–°å¢ã€‘åˆ—å‡ºå›æ”¶ç«™çš„APIï¼Œç”¨äºå­ç›®å½•é‡å­˜
    def recycle_list(self, page=1, size=50):
        params = {"_page": page, "_size": size, "pr": "ucpro", "fr": "pc"}
        response = self._send_request("GET", f"{self.BASE_URL}/1/clouddrive/file/recycle/list", params=params).json()
        if response.get("code") != 0: raise QuarkAPIError(f"åˆ—å‡ºå›æ”¶ç«™å¤±è´¥: {response.get('message')}")
        return response["data"]["list"]

    # ã€æ–°å¢ã€‘ä»å›æ”¶ç«™å½»åº•åˆ é™¤çš„APIï¼Œç”¨äºå­ç›®å½•é‡å­˜
    def recycle_remove(self, record_list):
        payload = {"select_mode": 2, "record_list": record_list}
        response = self._send_request("POST", f"{self.BASE_URL}/1/clouddrive/file/recycle/remove", json=payload, params={"pr": "ucpro", "fr": "pc"}).json()
        if response.get("code") != 0: raise QuarkAPIError(f"æ¸…ç†å›æ”¶ç«™å¤±è´¥: {response.get('message')}")
        return response

    def extract_url(self, url):
        match_id = re.search(r"/s/(\w+)", url)
        pwd_id = match_id.group(1) if match_id else None
        match_pwd = re.search(r"pwd=(\w+)", url)
        passcode = match_pwd.group(1) if match_pwd else ""
        pdir_fid = "0"
        if match := re.search(r"/(\w{32})", url.split('#')[-1]):
            pdir_fid = match.group(1)
        return pwd_id, passcode, pdir_fid

    def update_savepath_fid(self, tasklist):
        dir_paths = list(set([re.sub(r"/{2,}", "/", f"/{item['savepath']}") for item in tasklist if not item.get("enddate") or datetime.now().date() <= datetime.strptime(item["enddate"], "%Y-%m-%d").date()]))
        if not dir_paths: return
        for path in dir_paths:
            if path == "/": continue
            try:
                action = lambda: self.mkdir(path)
                dir_info = execute_with_retry(action, f"æ£€æŸ¥æˆ–åˆ›å»ºç›®å½• '{path}'")
                self.savepath_fid[path] = dir_info['fid']
                print(f"   âœ… [ç›®å½•æ£€æŸ¥] è·¯å¾„ '{path}' fid: {dir_info['fid']}")
            except QuarkAPIError as e:
                 print(f"   âŒ [ç›®å½•æ£€æŸ¥] è·¯å¾„ '{path}' å¤„ç†å¤±è´¥: {e}")
    
    def do_save_task(self, task):
        try:
            if task.get("shareurl_ban"):
                print(f"ã€Š{task['taskname']}ã€‹ï¼š{task['shareurl_ban']}")
                return

            pwd_id, passcode, pdir_fid = self.extract_url(task["shareurl"])
            stoken = execute_with_retry(lambda: self.get_stoken(pwd_id, passcode), "è·å–stoken")

            result_tree = Tree()
            mr = MagicRename(CONFIG_DATA.get("magic_regex", {}))
            mr.set_taskname(task["taskname"])
            
            root_save_path = re.sub(r"/{2,}", "/", f"/{task['savepath']}")
            print(f"â–¶ï¸ å¼€å§‹ä»åˆ†äº«æ ¹ç›®å½•è¿›è¡Œæ£€æŸ¥...")
            
            is_full_save = self._recursive_process_and_save(task, pwd_id, stoken, pdir_fid, root_save_path, result_tree, mr, is_root=True)

            if result_tree.size(1) > 0:
                notify_title = f"âœ…ã€Š{task['taskname']}ã€‹**{'å®Œæ•´ä¿å­˜' if is_full_save else 'æ·»åŠ è¿½æ›´'}**ï¼š"
                self.do_rename(result_tree)
                print()
                add_notify(f"{notify_title}\n{result_tree}")
                return result_tree
            else:
                print(f"ä»»åŠ¡ç»“æŸï¼šæ²¡æœ‰æ–°çš„è½¬å­˜æˆ–ä¿å­˜ä»»åŠ¡")
                return False

        except Exception as e:
            add_notify(f"âŒã€Š{task['taskname']}ã€‹æ‰§è¡Œå¤±è´¥ï¼Œå‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
            if isinstance(e, QuarkAPIError) and "è®¿é—®æ¬¡æ•°" not in str(e) and "ç½‘ç»œ" not in str(e):
                task["shareurl_ban"] = str(e)
            return False

    # ã€æ–°å¢ã€‘æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯é‡å¤æ–‡ä»¶ï¼ˆå¦‚å¸¦æœ‰(1)ã€(2)ç­‰åç¼€çš„æ–‡ä»¶ï¼‰
    def is_duplicate_file(self, filename):
        # åˆ†ç¦»æ–‡ä»¶åå’Œæ‰©å±•å
        name_part, ext_part = os.path.splitext(filename)
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«(æ•°å­—)æ¨¡å¼ï¼Œè¿™é€šå¸¸æ˜¯å¤¸å…‹ç½‘ç›˜è‡ªåŠ¨é‡å‘½åçš„ç»“æœ
        match = re.search(r"(.*?)\s*\((\d+)\)$", name_part)
        if match:
            # æå–åŸå§‹æ–‡ä»¶åï¼ˆæ²¡æœ‰æ•°å­—åç¼€çš„éƒ¨åˆ†ï¼‰
            base_name = match.group(1).strip() + ext_part
            return True, base_name
        return False, filename

    def _recursive_process_and_save(self, task, pwd_id, stoken, share_pdir_fid, current_save_path, result_tree, mr: MagicRename, is_root=False):
        print(f"\n   ğŸ” [å¼€å§‹å¤„ç†ç›®å½•] '{current_save_path}'")
        print(f"   ğŸ“ [å‚æ•°ä¿¡æ¯] pwd_id={pwd_id}, share_pdir_fid={share_pdir_fid}, is_root={is_root}")
        
        try:
            share_content_raw = execute_with_retry(lambda: self.get_detail(pwd_id, stoken, share_pdir_fid), f"è·å–åˆ†äº«å†…å®¹ '{current_save_path}'")
            
            # ã€æ–°å¢ã€‘å¤„ç†ç©ºåˆ†äº«é“¾æ¥çš„æƒ…å†µ
            if not share_content_raw:
                print(f"   âš ï¸ [åˆ†äº«å†…å®¹ä¸ºç©º] '{current_save_path}'")
                if is_root:
                    task["shareurl_ban"] = "åˆ†äº«ä¸ºç©ºï¼Œæ–‡ä»¶å¯èƒ½å·²è¢«åˆ†äº«è€…åˆ é™¤"
                    add_notify(f"âŒã€Š{task['taskname']}ã€‹ï¼š{task['shareurl_ban']}")
                    return False
                else:
                    print(f"   â­ï¸ [è·³è¿‡ç©ºç›®å½•] '{current_save_path}'")
                    return False
                
            print(f"   âœ… [è·å–åˆ†äº«å†…å®¹] '{current_save_path}' æˆåŠŸï¼Œå…± {len(share_content_raw)} ä¸ªé¡¹ç›®")
        except Exception as e:
            print(f"   âŒ [è·å–åˆ†äº«å†…å®¹å¤±è´¥] '{current_save_path}': {type(e).__name__}: {str(e)}")
            if is_root:
                task["shareurl_ban"] = f"è·å–åˆ†äº«å†…å®¹å¤±è´¥: {str(e)}"
                add_notify(f"âŒã€Š{task['taskname']}ã€‹ï¼š{task['shareurl_ban']}")
            return False

        dir_info = execute_with_retry(lambda: self.mkdir(current_save_path), f"ç¡®ä¿ç›®å½•å­˜åœ¨ '{current_save_path}'")
        target_dir_fid = dir_info['fid']
        self.savepath_fid[current_save_path] = target_dir_fid
        target_dir_content = execute_with_retry(lambda: self.ls_dir(target_dir_fid), f"åˆ—å‡ºç›®æ ‡ç›®å½• '{current_save_path}'")
        target_dir_filenames = [f["file_name"] for f in target_dir_content]
        print(f"   ğŸ“‚ [ç›®æ ‡ç›®å½•] '{current_save_path}' åŒ…å« {len(target_dir_content)} ä¸ªæ–‡ä»¶/æ–‡ä»¶å¤¹")

        # ã€æ–°å¢ã€‘åˆ›å»ºä¸€ä¸ªæ˜ å°„è¡¨ï¼Œå°†åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ•°å­—åç¼€ï¼‰æ˜ å°„åˆ°ç›®æ ‡ç›®å½•ä¸­çš„å®é™…æ–‡ä»¶å
        original_to_actual = {}
        for filename in target_dir_filenames:
            is_dup, base_name = self.is_duplicate_file(filename)
            original_to_actual[base_name] = original_to_actual.get(base_name, []) + [filename]

        is_full_save_mode = is_root and not target_dir_content
        if is_root:
            print(f"   â„¹ï¸ [æ¨¡å¼åˆ¤æ–­] ç›®æ ‡ç›®å½• '{current_save_path}' {'ä¸ºç©ºï¼Œåˆ¤å®šä¸ºå®Œæ•´ä¿å­˜æ¨¡å¼' if is_full_save_mode else 'å·²æœ‰å†…å®¹ï¼Œåˆ¤å®šä¸ºè¿½æ›´æ¨¡å¼'}")

        # è·å–å…¨å±€æ­£åˆ™é…ç½®
        global_regex_enabled = CONFIG_DATA.get("global_regex", {}).get("enabled", False)
        global_pattern = CONFIG_DATA.get("global_regex", {}).get("pattern", "")
        global_replace = CONFIG_DATA.get("global_regex", {}).get("replace", "")

        files_to_save = []
        for item in share_content_raw:
            item_name = item["file_name"]

            # é»‘åå•æ£€æŸ¥åº”è¯¥åœ¨æ‰€æœ‰å¤„ç†ä¹‹å‰è¿›è¡Œ
            if item_name in self.file_blacklist:
                print(f"   â­ï¸ [é»‘åå•] è·³è¿‡: '{current_save_path}/{item_name}'")
                continue
                
            # ã€æ”¹è¿›ã€‘æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ–‡ä»¶
            is_dup, base_name = self.is_duplicate_file(item_name)
            if is_dup:
                print(f"   â­ï¸ [é‡å¤æ–‡ä»¶] è·³è¿‡: '{current_save_path}/{item_name}'ï¼ŒåŸå§‹æ–‡ä»¶åä¸º: '{base_name}'")
                continue
                
            # ã€æ”¹è¿›ã€‘æ£€æŸ¥åŸå§‹æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŒ…æ‹¬å¯èƒ½å¸¦æœ‰æ•°å­—åç¼€çš„ç‰ˆæœ¬ï¼‰
            if base_name in original_to_actual:
                print(f"   â­ï¸ [æ–‡ä»¶å·²å­˜åœ¨] è·³è¿‡: '{current_save_path}/{item_name}'ï¼Œå·²å­˜åœ¨çš„æ–‡ä»¶: {original_to_actual[base_name]}")
                continue

            if item["dir"]:
                if task.get("update_subdir") and not re.search(task["update_subdir"], item_name):
                     print(f"   â­ï¸ [ç›®å½•ä¸åŒ¹é…] è·³è¿‡å­ç›®å½•: '{item_name}'")
                     continue
                
                # ã€æ–°å¢ã€‘å¤„ç†å­ç›®å½•é‡å­˜æ¨¡å¼ (resave mode)
                is_dir_exists = item_name in target_dir_filenames
                if is_dir_exists and task.get("update_subdir_resave_mode"):
                    print(f"   ğŸ”„ [é‡å­˜æ¨¡å¼] å‘ç°å·²å­˜åœ¨ç›®å½• '{item_name}', å‡†å¤‡æ‰§è¡Œåˆ é™¤åé‡å­˜ã€‚")
                    try:
                        # æ‰¾åˆ°æ—§ç›®å½•çš„fidå¹¶åˆ é™¤
                        old_dir_info = next((f for f in target_dir_content if f["file_name"] == item_name), None)
                        if old_dir_info:
                            del_task_info = execute_with_retry(lambda: self.delete([old_dir_info["fid"]]), f"åˆ é™¤æ—§ç›®å½• '{item_name}'")
                            execute_with_retry(lambda: self.query_task(del_task_info["task_id"]), "æŸ¥è¯¢åˆ é™¤ä»»åŠ¡çŠ¶æ€")
                            
                            # ä»å›æ”¶ç«™å½»åº•åˆ é™¤
                            recycles = self.recycle_list()
                            record_to_remove = [r['record_id'] for r in recycles if r['fid'] == old_dir_info['fid']]
                            if record_to_remove:
                                execute_with_retry(lambda: self.recycle_remove(record_to_remove), f"ä»å›æ”¶ç«™æ¸…ç† '{item_name}'")
                            print(f"   âœ… [é‡å­˜æ¨¡å¼] æ—§ç›®å½• '{item_name}' å·²å½»åº•åˆ é™¤ã€‚")
                            
                            # å°†å…¶ä½œä¸ºæ–°æ–‡ä»¶åŠ å…¥å¾…ä¿å­˜åˆ—è¡¨
                            item["file_name_re"] = item_name
                            files_to_save.append(item)
                    except Exception as e:
                        print(f"   âŒ [é‡å­˜å¤±è´¥] å¤„ç†ç›®å½• '{item_name}' å¤±è´¥: {e}")
                    continue # ä¸å†é€’å½’è¿›å…¥

                print(f"   â–¶ï¸ [æ·±å…¥ç›®å½•] æ­£åœ¨æ£€æŸ¥: '{current_save_path}/{item_name}'")
                self._recursive_process_and_save(task, pwd_id, stoken, item["fid"], f"{current_save_path}/{item_name}", result_tree, mr)
                continue

            # åˆå§‹åŒ–é‡å‘½ååçš„æ–‡ä»¶åä¸ºåŸå§‹æ–‡ä»¶å
            renamed_name = item_name
            applied_global_regex = False
            applied_task_regex = False

            # åº”ç”¨å…¨å±€æ­£åˆ™ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if global_regex_enabled and global_pattern:
                if re.search(global_pattern, renamed_name):
                    global_renamed_name = re.sub(global_pattern, global_replace, renamed_name)
                    print(f"   ğŸŒ [å…¨å±€æ­£åˆ™] '{renamed_name}' -> '{global_renamed_name}'")
                    renamed_name = global_renamed_name
                    applied_global_regex = True

            # åº”ç”¨ä»»åŠ¡æ­£åˆ™ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            task_pattern, task_replace = mr.magic_regex_conv(task.get("pattern", ""), task.get("replace", ""))
            if task_pattern:
                if re.search(task_pattern, renamed_name):
                    task_renamed_name = mr.sub(task_pattern, task_replace, renamed_name)
                    print(f"   ğŸ”§ [ä»»åŠ¡æ­£åˆ™] '{renamed_name}' -> '{task_renamed_name}'")
                    renamed_name = task_renamed_name
                    applied_task_regex = True

            # ã€æ”¹è¿›ã€‘æ£€æŸ¥é‡å‘½ååçš„æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            is_dup_renamed, base_name_renamed = self.is_duplicate_file(renamed_name)
            if is_dup_renamed:
                print(f"   â­ï¸ [é‡å‘½ååä¸ºé‡å¤æ–‡ä»¶] è·³è¿‡: '{renamed_name}'ï¼ŒåŸå§‹æ–‡ä»¶åä¸º: '{base_name_renamed}'")
                continue
                
            if base_name_renamed in original_to_actual:
                print(f"   â­ï¸ [é‡å‘½ååæ–‡ä»¶å·²å­˜åœ¨] è·³è¿‡: '{renamed_name}'ï¼Œå·²å­˜åœ¨çš„æ–‡ä»¶: {original_to_actual[base_name_renamed]}")
                continue

            # æ£€æŸ¥æ˜¯å¦åº”ç”¨äº†ä»»ä½•æ­£åˆ™
            if applied_global_regex or applied_task_regex:
                original_exists = mr.is_exists(item_name, target_dir_filenames, task.get("ignore_extension"))
                renamed_exists = mr.is_exists(renamed_name, target_dir_filenames, task.get("ignore_extension"))
                if not original_exists and not renamed_exists:
                    print(f"   ğŸ“¥ [å¾…å¤„ç†] æ·»åŠ åˆ°åˆ—è¡¨: '{item_name}' -> '{renamed_name}'")
                    item["file_name_re"] = renamed_name
                    files_to_save.append(item)
                else:
                    print(f"   â­ï¸ [å·²å­˜åœ¨] è·³è¿‡: '{item_name}' (æœ¬åœ°å­˜åœ¨: '{original_exists or renamed_exists}')")
            else:
                print(f"   â­ï¸ [æ­£åˆ™ä¸åŒ¹é…] è·³è¿‡: '{item_name}'")

        if files_to_save:
            # å¤„ç†æ–‡ä»¶åä¸­çš„åºå·å˜é‡ {I}
            replace_for_i = ""
            if task.get("replace"):
                replace_for_i = mr.magic_regex_conv(task.get("pattern", ""), task.get("replace", ""))[1]
            elif global_regex_enabled and global_replace:
                replace_for_i = global_replace
                
            if replace_for_i and re.search(r"\{I+\}", replace_for_i):
                mr.set_dir_file_list(target_dir_content, replace_for_i)
                mr.sort_file_list(files_to_save)

            action_save = lambda: self.save_file([f["fid"] for f in files_to_save], [f["share_fid_token"] for f in files_to_save], target_dir_fid, pwd_id, stoken)
            query_ret = execute_with_retry(action_save, f"ä¿å­˜åœ¨ '{current_save_path}' ä¸­çš„ {len(files_to_save)} ä¸ªæ–‡ä»¶")
            saved_fids = query_ret["save_as"]["save_as_top_fids"]

            if not result_tree.nodes:
                result_tree.create_node(current_save_path, target_dir_fid, data={"is_dir": True, "full_save": is_full_save_mode})
            parent_node_id = target_dir_fid
            if not result_tree.contains(parent_node_id):
                parent_path, dir_name = os.path.split(current_save_path)
                parent_dir_fid = self.savepath_fid.get(parent_path)
                parent_node_id_to_attach = parent_dir_fid if parent_dir_fid and result_tree.contains(parent_dir_fid) else result_tree.root
                result_tree.create_node(f"ğŸ“{dir_name}", parent_node_id, parent=parent_node_id_to_attach, data={"is_dir": True})

            for i, item in enumerate(files_to_save):
                new_fid = saved_fids[i]
                result_tree.create_node(f"{self._get_file_icon(item)}{item['file_name_re']}", new_fid, parent=parent_node_id,
                                        data={"file_name": item["file_name"], "file_name_re": item["file_name_re"], "fid": new_fid, "is_dir": False, "obj_category": item.get("obj_category", "")})
        return is_full_save_mode

    def do_rename(self, tree, node_id=None):
        if node_id is None: node_id = tree.root
        for child in list(tree.children(node_id)): self.do_rename(tree, child.identifier)
        node = tree.get_node(node_id)
        if not node or node.data.get("is_dir"): return
        file = node.data
        if file and file.get("file_name_re") and file["file_name_re"] != file["file_name"]:
            action = lambda: self.rename(file["fid"], file["file_name_re"])
            try:
                execute_with_retry(action, f"é‡å‘½å '{file['file_name']}'")
                print(f"é‡å‘½åæˆåŠŸï¼š'{file['file_name']}' â†’ '{file['file_name_re']}'")
            except Exception as e:
                print(f"é‡å‘½åå¤±è´¥ï¼š'{file['file_name']}' â†’ '{file['file_name_re']}', åŸå› : {e}")

    def _get_file_icon(self, f):
        if f.get("dir"): return "ğŸ“"
        return {"video": "ğŸï¸", "image": "ğŸ–¼ï¸", "audio": "ğŸµ", "doc": "ğŸ“„", "archive": "ğŸ“¦"}.get(f.get("obj_category"), "")

    def get_fids(self, path_list):
        """
        æ ¹æ®è·¯å¾„åˆ—è¡¨è·å–å¯¹åº”çš„æ–‡ä»¶ID
        :param path_list: è·¯å¾„åˆ—è¡¨ï¼Œå¦‚ ['/dir1', '/dir1/dir2']
        :return: åŒ…å«è·¯å¾„å’Œæ–‡ä»¶IDçš„åˆ—è¡¨
        """
        result = []
        for path in path_list:
            if path in self.savepath_fid:
                result.append({"path": path, "fid": self.savepath_fid[path]})
                continue
                
            try:
                dir_info = execute_with_retry(
                    lambda: self.mkdir(path),
                    f"æ£€æŸ¥æˆ–åˆ›å»ºç›®å½• '{path}'"
                )
                self.savepath_fid[path] = dir_info['fid']
                result.append({"path": path, "fid": dir_info['fid']})
            except Exception as e:
                print(f"   âŒ [è·å–ç›®å½•ID] è·¯å¾„ '{path}' å¤„ç†å¤±è´¥: {e}")
                return None
                
        return result


def verify_account(account):
    try:
        action = lambda: account.init()
        execute_with_retry(action, f"éªŒè¯è´¦å· {account.index}")
        print(f"ğŸ‘¤ è´¦å·æ˜µç§°: {account.nickname}âœ…")
        return True
    except Exception as e:
        add_notify(f"ğŸ‘¤ ç¬¬{account.index}ä¸ªè´¦å·ç™»å½•å¤±è´¥: {e}âŒ")
        return False

# ã€æ–°å¢ã€‘æ¯æ—¥ç­¾åˆ°å‡½æ•°
def do_sign(account):
    if not account.mparam:
        print(f"â­ï¸ è´¦å· {account.nickname} ç§»åŠ¨ç«¯å‚æ•°æœªè®¾ç½®ï¼Œè·³è¿‡ç­¾åˆ°")
        return
    print(f"â–¶ï¸ è´¦å· {account.nickname} å¼€å§‹æ‰§è¡Œæ¯æ—¥ç­¾åˆ°...")
    try:
        growth_info = account.get_growth_info()
        if not growth_info:
            print(f"   âŒ è·å–ç­¾åˆ°ä¿¡æ¯å¤±è´¥")
            return

        growth_message = (
            f"ğŸ’¾ {'88VIP' if growth_info.get('88VIP') else 'æ™®é€šç”¨æˆ·'} "
            f"æ€»ç©ºé—´ï¼š{format_bytes(growth_info.get('total_capacity', 0))}ï¼Œ"
            f"ç­¾åˆ°ç´¯è®¡è·å¾—ï¼š{format_bytes(growth_info.get('cap_composition', {}).get('sign_reward', 0))}"
        )

        if growth_info.get("cap_sign", {}).get("sign_daily"):
            sign_progress = growth_info.get('cap_sign', {}).get('sign_progress', 0)
            sign_target = growth_info.get('cap_sign', {}).get('sign_target', 7)
            sign_message = f"   ğŸ“… ä»Šæ—¥å·²ç­¾åˆ°ï¼Œè¿ç­¾è¿›åº¦({sign_progress}/{sign_target})âœ…"
            print(f"{sign_message}\n   {growth_message}")
        else:
            signed, reward = account.get_growth_sign()
            if signed:
                sign_progress = growth_info.get('cap_sign', {}).get('sign_progress', 0) + 1
                sign_target = growth_info.get('cap_sign', {}).get('sign_target', 7)
                reward_mb = int(reward) / 1024 / 1024 if isinstance(reward, int) else 0
                sign_message = f"   ğŸ“… æ‰§è¡Œç­¾åˆ°æˆåŠŸ! è·å¾— {reward_mb:.0f}MB ç©ºé—´ï¼Œè¿ç­¾è¿›åº¦({sign_progress}/{sign_target})âœ…"
                message_to_notify = f"[{account.nickname}] ç­¾åˆ°æˆåŠŸ: +{reward_mb:.0f}MB\n{growth_message}"
                print(f"{sign_message}\n   {growth_message}")

                push_sign_notify = str(CONFIG_DATA.get("push_config", {}).get("QUARK_SIGN_NOTIFY", "true")).lower() != "false" and \
                                   os.environ.get("QUARK_SIGN_NOTIFY", "true").lower() != "false"
                if push_sign_notify:
                    add_notify(message_to_notify)
            else:
                print(f"   âŒ ç­¾åˆ°å¤±è´¥: {reward}")
    except Exception as e:
        print(f"   âŒ ç­¾åˆ°æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")

def do_save(account, tasklist=[]):
    print(f"ğŸ§© è½½å…¥æ’ä»¶...")
    plugins, _, _ = Config.load_plugins(CONFIG_DATA.get("plugins", {}))
    print(f"è½¬å­˜è´¦å·: {account.nickname}")
    try:
        account.update_savepath_fid(tasklist)
    except Exception as e:
        print(f"âŒ æ›´æ–°ä¿å­˜è·¯å¾„å¤±è´¥ï¼Œä»»åŠ¡ä¸­æ­¢: {e}")
        return

    def is_time(task):
        return (not task.get("enddate") or datetime.now().date() <= datetime.strptime(task["enddate"], "%Y-%m-%d").date()) and \
               ("runweek" not in task or datetime.today().weekday() + 1 in task.get("runweek"))

    for index, task in enumerate(tasklist):
        print(f"\n#{index+1}------------------\nä»»åŠ¡åç§°: {task['taskname']}\nåˆ†äº«é“¾æ¥: {task['shareurl']}\nä¿å­˜è·¯å¾„: {task['savepath']}")
        if task.get("pattern"): print(f"æ­£åˆ™åŒ¹é…: {task['pattern']}")
        if task.get("replace"): print(f"æ­£åˆ™æ›¿æ¢: {task['replace']}")
        if task.get("update_subdir"): print(f"æ›´å­ç›®å½•: {task['update_subdir']}")
        print()

        if not is_time(task):
            print(f"ä»»åŠ¡ä¸åœ¨è¿è¡Œå‘¨æœŸå†…ï¼Œè·³è¿‡")
            continue
        
        is_new_tree = account.do_save_task(task)

        if is_new_tree:
            print(f"ğŸ§© è°ƒç”¨æ’ä»¶")
            for plugin_name, plugin in plugins.items():
                if plugin.is_active:
                    task = plugin.run(task, account=account, tree=is_new_tree) or task

def main():
    global CONFIG_DATA
    start_time = datetime.now()
    print(f"===============ç¨‹åºå¼€å§‹===============\nâ° æ‰§è¡Œæ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    config_path = sys.argv[1] if len(sys.argv) > 1 else "quark_config.json"
    
    if os.environ.get("QUARK_TEST", "").lower() == "true":
        print("===============é€šçŸ¥æµ‹è¯•===============")
        CONFIG_DATA["push_config"] = json.loads(os.environ.get("PUSH_CONFIG", "{}"))
        send_ql_notify("ã€å¤¸å…‹è‡ªåŠ¨è½¬å­˜ã€‘", f"é€šçŸ¥æµ‹è¯•\n\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        return

    tasklist_from_env = []
    if tasklist_json := os.environ.get("TASKLIST"):
        try: tasklist_from_env = json.loads(tasklist_json)
        except Exception as e: print(f"ä»ç¯å¢ƒå˜é‡è§£æä»»åŠ¡åˆ—è¡¨å¤±è´¥ {e}")

    cookie_form_file = True
    if not os.path.exists(config_path):
        if os.environ.get("QUARK_COOKIE"):
            print(f"âš™ï¸ è¯»å–åˆ° QUARK_COOKIE ç¯å¢ƒå˜é‡ï¼Œå°†ä»…æ‰§è¡Œç­¾åˆ°ã€‚")
            cookie_val = os.environ.get("QUARK_COOKIE")
            cookie_form_file = False
        else:
            print(f"âš™ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œæ­£è¿œç¨‹ä¸‹è½½é…ç½®æ¨¡ç‰ˆ...")
            if Config.download_file(f"{GH_PROXY}https://raw.githubusercontent.com/Cp0204/quark_auto_save/main/quark_config.json", config_path):
                print("âš™ï¸ é…ç½®æ¨¡ç‰ˆä¸‹è½½æˆåŠŸï¼Œè¯·åˆ°ç¨‹åºç›®å½•ä¸­æ‰‹åŠ¨é…ç½®")
            return
    else:
        print(f"âš™ï¸ æ­£ä» {config_path} æ–‡ä»¶ä¸­è¯»å–é…ç½®")
        CONFIG_DATA = Config.read_json(config_path)
        Config.breaking_change_update(CONFIG_DATA)
        cookie_val = CONFIG_DATA.get("cookie")

    cookies = Config.get_cookies(cookie_val)
    if not cookies:
        print("âŒ cookie æœªé…ç½®")
        return
    
    accounts = [Quark(cookie, index, blacklist=CONFIG_DATA.get("file_blacklist", [])) for index, cookie in enumerate(cookies)]

    print("\n===============è´¦å·éªŒè¯ä¸ç­¾åˆ°===============")
    valid_accounts = []
    for acc in accounts:
        if verify_account(acc):
            valid_accounts.append(acc)
            # ã€æ–°å¢ã€‘å¯¹æ¯ä¸ªéªŒè¯é€šè¿‡çš„è´¦å·æ‰§è¡Œç­¾åˆ°
            do_sign(acc)
        print()

    if not valid_accounts:
        print("æ— æœ‰æ•ˆè´¦å·ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # ã€ä¼˜åŒ–ã€‘å³ä½¿ä»ç¯å¢ƒå˜é‡è¯»å–ä»»åŠ¡ï¼Œä¹Ÿä½¿ç”¨æ–‡ä»¶ä¸­çš„é…ç½®
    if cookie_form_file or tasklist_from_env:
        print("\n===============è½¬å­˜ä»»åŠ¡===============")
        # ä½ çš„åŸé€»è¾‘æ˜¯åªç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆè´¦å·æ‰§è¡Œè½¬å­˜ï¼Œè¿™é‡Œäºˆä»¥ä¿ç•™
        do_save(valid_accounts[0], tasklist_from_env or CONFIG_DATA.get("tasklist", []))

    if NOTIFYS:
        print("\n===============æ¨é€é€šçŸ¥===============")
        send_ql_notify("ã€å¤¸å…‹è‡ªåŠ¨è½¬å­˜ã€‘", "\n".join(NOTIFYS))
    
    if cookie_form_file:
        Config.write_json(config_path, CONFIG_DATA)

    duration = datetime.now() - start_time
    print(f"\n===============ç¨‹åºç»“æŸ===============\nğŸ˜ƒ è¿è¡Œæ—¶é•¿: {round(duration.total_seconds(), 2)}s\n")


if __name__ == "__main__":
    main()
