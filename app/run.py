# !/usr/bin/env python3
# -*- coding: utf-8 -*-
from flask import (
    json,
    Flask,
    url_for,
    session,
    jsonify,
    request,
    redirect,
    Response,
    render_template,
    send_from_directory,
    stream_with_context,
)
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from sdk.cloudsaver import CloudSaver
from datetime import timedelta, datetime
import subprocess
import requests
import hashlib
import logging
import base64
import sys
import os
import re
import random
import time
import treelib
from functools import lru_cache
from threading import Lock

parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, parent_dir)
from quark_auto_save import Quark
from quark_auto_save import Config, format_bytes

# æ·»åŠ å¯¼å…¥å…¨å±€extract_episode_numberå’Œsort_file_by_nameå‡½æ•°
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from quark_auto_save import extract_episode_number, sort_file_by_name, chinese_to_arabic, is_date_format

# å¯¼å…¥è±†ç“£æœåŠ¡
from sdk.douban_service import douban_service

def advanced_filter_files(file_list, filterwords):
    """
    é«˜çº§è¿‡æ»¤å‡½æ•°ï¼Œæ”¯æŒä¿ç•™è¯å’Œè¿‡æ»¤è¯
    
    Args:
        file_list: æ–‡ä»¶åˆ—è¡¨
        filterwords: è¿‡æ»¤è§„åˆ™å­—ç¬¦ä¸²ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
            - "åŠ æ›´ï¼Œä¼åˆ’ï¼Œè¶…å‰ï¼Œ(1)ï¼Œmkvï¼Œnfo"  # åªæœ‰è¿‡æ»¤è¯
            - "æœŸ|åŠ æ›´ï¼Œä¼åˆ’ï¼Œè¶…å‰ï¼Œ(1)ï¼Œmkvï¼Œnfo"  # ä¿ç•™è¯|è¿‡æ»¤è¯
            - "æœŸï¼Œ2160P|åŠ æ›´ï¼Œä¼åˆ’ï¼Œè¶…å‰ï¼Œ(1)ï¼Œmkvï¼Œnfo"  # å¤šä¸ªä¿ç•™è¯(æˆ–å…³ç³»)|è¿‡æ»¤è¯
            - "æœŸ|2160P|åŠ æ›´ï¼Œä¼åˆ’ï¼Œè¶…å‰ï¼Œ(1)ï¼Œmkvï¼Œnfo"  # å¤šä¸ªä¿ç•™è¯(å¹¶å…³ç³»)|è¿‡æ»¤è¯
            - "æœŸï¼Œ2160P|"  # åªæœ‰ä¿ç•™è¯ï¼Œæ— è¿‡æ»¤è¯
    
    Returns:
        è¿‡æ»¤åçš„æ–‡ä»¶åˆ—è¡¨
    """
    if not filterwords or not filterwords.strip():
        return file_list
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ†éš”ç¬¦ |
    if '|' not in filterwords:
        # åªæœ‰è¿‡æ»¤è¯çš„æƒ…å†µ
        filterwords = filterwords.replace("ï¼Œ", ",")
        filterwords_list = [word.strip().lower() for word in filterwords.split(',') if word.strip()]
        
        filtered_files = []
        for file in file_list:
            file_name = file['file_name'].lower()
            file_ext = os.path.splitext(file_name)[1].lower().lstrip('.')
            
            # æ£€æŸ¥è¿‡æ»¤è¯æ˜¯å¦å­˜åœ¨äºæ–‡ä»¶åä¸­ï¼Œæˆ–è€…è¿‡æ»¤è¯ç­‰äºæ‰©å±•å
            if not any(word in file_name for word in filterwords_list) and not any(word == file_ext for word in filterwords_list):
                filtered_files.append(file)
        
        return filtered_files
    
    # åŒ…å«åˆ†éš”ç¬¦çš„æƒ…å†µï¼Œéœ€è¦è§£æä¿ç•™è¯å’Œè¿‡æ»¤è¯
    parts = filterwords.split('|')
    if len(parts) < 2:
        # æ ¼å¼é”™è¯¯ï¼Œè¿”å›åŸåˆ—è¡¨
        return file_list
    
    # æœ€åä¸€ä¸ª|åé¢çš„æ˜¯è¿‡æ»¤è¯
    filter_part = parts[-1].strip()
    # å‰é¢çš„éƒ½æ˜¯ä¿ç•™è¯
    keep_parts = [part.strip() for part in parts[:-1] if part.strip()]
    
    # è§£æè¿‡æ»¤è¯
    filterwords_list = []
    if filter_part:
        filter_part = filter_part.replace("ï¼Œ", ",")
        filterwords_list = [word.strip().lower() for word in filter_part.split(',') if word.strip()]
    
    # è§£æä¿ç•™è¯ï¼šæ¯ä¸ª|åˆ†éš”çš„éƒ¨åˆ†éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç­›é€‰æ¡ä»¶
    # è¿™äº›æ¡ä»¶éœ€è¦æŒ‰é¡ºåºä¾æ¬¡åº”ç”¨ï¼Œå½¢æˆé“¾å¼ç­›é€‰
    keep_conditions = []
    for part in keep_parts:
        if part.strip():
            if ',' in part or 'ï¼Œ' in part:
                # åŒ…å«é€—å·ï¼Œè¡¨ç¤ºæˆ–å…³ç³»
                part = part.replace("ï¼Œ", ",")
                or_words = [word.strip().lower() for word in part.split(',') if word.strip()]
                keep_conditions.append(("or", or_words))
            else:
                # ä¸åŒ…å«é€—å·ï¼Œè¡¨ç¤ºå•ä¸ªè¯
                keep_conditions.append(("single", [part.strip().lower()]))
    
    # ç¬¬ä¸€æ­¥ï¼šåº”ç”¨ä¿ç•™è¯ç­›é€‰ï¼ˆé“¾å¼ç­›é€‰ï¼‰
    if keep_conditions:
        for condition_type, words in keep_conditions:
            filtered_by_keep = []
            for file in file_list:
                file_name = file['file_name'].lower()
                
                if condition_type == "or":
                    # æˆ–å…³ç³»ï¼šåŒ…å«ä»»æ„ä¸€ä¸ªè¯å³å¯
                    if any(word in file_name for word in words):
                        filtered_by_keep.append(file)
                elif condition_type == "single":
                    # å•ä¸ªè¯ï¼šå¿…é¡»åŒ…å«
                    if words[0] in file_name:
                        filtered_by_keep.append(file)
            
            file_list = filtered_by_keep
    
    # ç¬¬äºŒæ­¥ï¼šåº”ç”¨è¿‡æ»¤è¯è¿‡æ»¤
    if filterwords_list:
        filtered_files = []
        for file in file_list:
            file_name = file['file_name'].lower()
            file_ext = os.path.splitext(file_name)[1].lower().lstrip('.')
            
            # æ£€æŸ¥è¿‡æ»¤è¯æ˜¯å¦å­˜åœ¨äºæ–‡ä»¶åä¸­ï¼Œæˆ–è€…è¿‡æ»¤è¯ç­‰äºæ‰©å±•å
            if not any(word in file_name for word in filterwords_list) and not any(word == file_ext for word in filterwords_list):
                filtered_files.append(file)
        
        return filtered_files
    
    return file_list


def process_season_episode_info(filename, task_name=None):
    """
    å¤„ç†æ–‡ä»¶åä¸­çš„å­£æ•°å’Œé›†æ•°ä¿¡æ¯

    Args:
        filename: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        task_name: å¯é€‰çš„ä»»åŠ¡åç§°ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦å·²åŒ…å«å­£æ•°ä¿¡æ¯

    Returns:
        å¤„ç†åçš„æ˜¾ç¤ºåç§°
    """

    def task_contains_season_info(task_name):
        """
        åˆ¤æ–­ä»»åŠ¡åç§°ä¸­æ˜¯å¦åŒ…å«å­£æ•°ä¿¡æ¯
        """
        if not task_name:
            return False

        # æ¸…ç†ä»»åŠ¡åç§°ä¸­çš„è¿ç»­ç©ºæ ¼å’Œç‰¹æ®Šç¬¦å·
        clean_name = task_name.replace('\u3000', ' ').replace('\t', ' ')
        clean_name = re.sub(r'\s+', ' ', clean_name).strip()

        # åŒ¹é…å¸¸è§çš„å­£æ•°æ ¼å¼
        season_patterns = [
            r'^(.*?)[\s\.\-_]+S\d+$',  # é»‘é•œ - S07ã€æŠ˜è…°.S01ã€éŸ³ä½ è€Œæ¥-S02
            r'^(.*?)[\s\.\-_]+Season\s*\d+$',  # é»‘é•œ - Season 1
            r'^(.*?)\s+S\d+$',  # å¿«ä¹çš„å¤§äºº S02
            r'^(.*?)[\s\.\-_]+S\d+E\d+$',  # å¤„ç† S01E01 æ ¼å¼
            r'^(.*?)\s+ç¬¬\s*\d+\s*å­£$',  # å¤„ç† ç¬¬Nå­£ æ ¼å¼
            r'^(.*?)[\s\.\-_]+ç¬¬\s*\d+\s*å­£$',  # å¤„ç† - ç¬¬Nå­£ æ ¼å¼
            r'^(.*?)\s+ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶]+å­£$',  # å¤„ç† ç¬¬ä¸€å­£ã€ç¬¬äºŒå­£ æ ¼å¼
            r'^(.*?)[\s\.\-_]+ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶]+å­£$',  # å¤„ç† - ç¬¬ä¸€å­£ã€- ç¬¬äºŒå­£ æ ¼å¼
        ]

        for pattern in season_patterns:
            if re.match(pattern, clean_name, re.IGNORECASE):
                return True

        return False

    # åŒ¹é… SxxExx æ ¼å¼ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    # æ”¯æŒ S1E1, S01E01, s13e10, S100E1000 ç­‰æ ¼å¼
    season_episode_match = re.search(r'[Ss](\d+)[Ee](\d+)', filename)
    if season_episode_match:
        season = season_episode_match.group(1).zfill(2)  # ç¡®ä¿ä¸¤ä½æ•°
        episode = season_episode_match.group(2).zfill(2)  # ç¡®ä¿ä¸¤ä½æ•°

        # å¦‚æœä»»åŠ¡åç§°ä¸­å·²åŒ…å«å­£æ•°ä¿¡æ¯ï¼Œåªæ˜¾ç¤ºé›†æ•°
        if task_contains_season_info(task_name):
            return f"E{episode}"
        else:
            return f"S{season}E{episode}"

    # åŒ¹é…åªæœ‰ Exx æˆ– EPxx æ ¼å¼ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    # æ”¯æŒ E1, E01, EP1, EP01, e10, ep10, E1000 ç­‰æ ¼å¼
    episode_only_match = re.search(r'[Ee][Pp]?(\d+)', filename)
    if episode_only_match:
        episode = episode_only_match.group(1).zfill(2)  # ç¡®ä¿ä¸¤ä½æ•°
        return f"E{episode}"

    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å­£æ•°é›†æ•°ä¿¡æ¯ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å»é™¤ä¸ä»»åŠ¡åç§°ç›¸åŒçš„å‰ç¼€
    if task_name:
        task_name_clean = task_name.strip()

        # é¦–å…ˆå°è¯•å®Œæ•´ä»»åŠ¡ååŒ¹é…
        if filename.startswith(task_name_clean):
            # å»é™¤ä»»åŠ¡åå‰ç¼€
            remaining = filename[len(task_name_clean):].strip()

            # å¦‚æœå‰©ä½™éƒ¨åˆ†ä»¥å¸¸è§åˆ†éš”ç¬¦å¼€å¤´ï¼Œä¹Ÿå»é™¤åˆ†éš”ç¬¦
            separators = [' - ', ' Â· ', '.', '_', '-', ' ']
            for sep in separators:
                if remaining.startswith(sep):
                    remaining = remaining[len(sep):].strip()
                    break

            # å¦‚æœå‰©ä½™éƒ¨åˆ†ä¸ä¸ºç©ºï¼Œè¿”å›å‰©ä½™éƒ¨åˆ†ï¼›å¦åˆ™è¿”å›åŸæ–‡ä»¶å
            return remaining if remaining else filename

        # å¦‚æœå®Œæ•´ä»»åŠ¡åä¸åŒ¹é…ï¼Œå°è¯•æå–å‰§åéƒ¨åˆ†è¿›è¡ŒåŒ¹é…
        # åŒ¹é…å¸¸è§çš„å­£æ•°æ ¼å¼ï¼Œæå–å‰§åéƒ¨åˆ†
        season_patterns = [
            r'^(.*?)[\s\.\-_]+S\d+$',  # ä½ å¥½ï¼Œæ˜ŸæœŸå…­ - S04
            r'^(.*?)[\s\.\-_]+Season\s*\d+$',  # é»‘é•œ - Season 1
            r'^(.*?)\s+S\d+$',  # å¿«ä¹çš„å¤§äºº S02
            r'^(.*?)[\s\.\-_]+S\d+E\d+$',  # å¤„ç† S01E01 æ ¼å¼
            r'^(.*?)\s+ç¬¬\s*\d+\s*å­£$',  # å¤„ç† ç¬¬Nå­£ æ ¼å¼
            r'^(.*?)[\s\.\-_]+ç¬¬\s*\d+\s*å­£$',  # å¤„ç† - ç¬¬Nå­£ æ ¼å¼
            r'^(.*?)\s+ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶]+å­£$',  # å¤„ç† ç¬¬ä¸€å­£ã€ç¬¬äºŒå­£ æ ¼å¼
            r'^(.*?)[\s\.\-_]+ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶]+å­£$',  # å¤„ç† - ç¬¬ä¸€å­£ã€- ç¬¬äºŒå­£ æ ¼å¼
        ]

        for pattern in season_patterns:
            match = re.match(pattern, task_name_clean, re.IGNORECASE)
            if match:
                show_name = match.group(1).strip()
                # å»é™¤æœ«å°¾å¯èƒ½æ®‹ç•™çš„åˆ†éš”ç¬¦
                show_name = re.sub(r'[\s\.\-_]+$', '', show_name)

                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä»¥å‰§åå¼€å¤´
                if filename.startswith(show_name):
                    # å»é™¤å‰§åå‰ç¼€
                    remaining = filename[len(show_name):].strip()

                    # å¦‚æœå‰©ä½™éƒ¨åˆ†ä»¥å¸¸è§åˆ†éš”ç¬¦å¼€å¤´ï¼Œä¹Ÿå»é™¤åˆ†éš”ç¬¦
                    separators = [' - ', ' Â· ', '.', '_', '-', ' ']
                    for sep in separators:
                        if remaining.startswith(sep):
                            remaining = remaining[len(sep):].strip()
                            break

                    # å¦‚æœå‰©ä½™éƒ¨åˆ†ä¸ä¸ºç©ºï¼Œè¿”å›å‰©ä½™éƒ¨åˆ†ï¼›å¦åˆ™è¿”å›åŸæ–‡ä»¶å
                    return remaining if remaining else filename

    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å­£æ•°é›†æ•°ä¿¡æ¯ï¼Œä¸”æ²¡æœ‰æ‰¾åˆ°ç›¸åŒå‰ç¼€ï¼Œè¿”å›åŸæ–‡ä»¶å
    return filename

# å¯¼å…¥æ‹¼éŸ³æ’åºå·¥å…·
try:
    from utils.pinyin_sort import get_filename_pinyin_sort_key
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å°å†™æ’åºä½œä¸ºå¤‡ç”¨
    def get_filename_pinyin_sort_key(filename):
        return filename.lower()

# å¯¼å…¥æ•°æ®åº“æ¨¡å—
try:
    # å…ˆå°è¯•ç›¸å¯¹å¯¼å…¥
    from sdk.db import RecordDB
except ImportError:
    try:
        # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ä»appåŒ…å¯¼å…¥
        from app.sdk.db import RecordDB
    except ImportError:
        # å¦‚æœæ²¡æœ‰æ•°æ®åº“æ¨¡å—ï¼Œå®šä¹‰ä¸€ä¸ªç©ºç±»
        class RecordDB:
            def __init__(self, *args, **kwargs):
                pass
            
            def get_records(self, *args, **kwargs):
                return {"records": [], "pagination": {"total_records": 0, "total_pages": 0, "current_page": 1, "page_size": 20}}

# å¯¼å…¥å·¥å…·å‡½æ•°
try:
    # å…ˆå°è¯•ç›¸å¯¹å¯¼å…¥
    from sdk.utils import format_bytes, get_file_icon, format_file_display
except ImportError:
    try:
        # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ä»appåŒ…å¯¼å…¥
        from app.sdk.utils import format_bytes, get_file_icon, format_file_display
    except ImportError:
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å®ç°æˆ–ä»quark_auto_saveå¯¼å…¥
        # format_byteså·²ä»quark_auto_saveå¯¼å…¥
        def get_file_icon(file_name, is_dir=False):
            return "ğŸ“„" if not is_dir else "ğŸ“"
            
        def format_file_display(prefix, icon, name):
            return f"{prefix}{icon} {name}"


def get_app_ver():
    BUILD_SHA = os.environ.get("BUILD_SHA", "")
    BUILD_TAG = os.environ.get("BUILD_TAG", "")
    if BUILD_TAG[:1] == "v":
        return BUILD_TAG
    elif BUILD_SHA:
        return f"{BUILD_TAG}({BUILD_SHA[:7]})"
    else:
        return "dev"


# æ–‡ä»¶è·¯å¾„
PYTHON_PATH = "python3" if os.path.exists("/usr/bin/python3") else "python"
SCRIPT_PATH = os.environ.get("SCRIPT_PATH", "./quark_auto_save.py")
CONFIG_PATH = os.environ.get("CONFIG_PATH", "./config/quark_config.json")
PLUGIN_FLAGS = os.environ.get("PLUGIN_FLAGS", "")
DEBUG = os.environ.get("DEBUG", "false").lower() == "true"
# ä»ç¯å¢ƒå˜é‡è·å–ç«¯å£ï¼Œé»˜è®¤ä¸º5005
PORT = int(os.environ.get("PORT", "5005"))

config_data = {}
task_plugins_config_default = {}

# æ–‡ä»¶åˆ—è¡¨ç¼“å­˜
file_list_cache = {}
cache_lock = Lock()

# é»˜è®¤æ€§èƒ½å‚æ•°ï¼ˆå¦‚æœé…ç½®ä¸­æ²¡æœ‰è®¾ç½®ï¼‰
DEFAULT_PERFORMANCE_CONFIG = {
    "api_page_size": 200,
    "cache_expire_time": 30
}

def get_performance_config():
    """è·å–æ€§èƒ½é…ç½®å‚æ•°"""
    try:
        if config_data and "file_performance" in config_data:
            perf_config = config_data["file_performance"]
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ•´æ•°ç±»å‹
            result = {}
            for key, default_value in DEFAULT_PERFORMANCE_CONFIG.items():
                try:
                    result[key] = int(perf_config.get(key, default_value))
                except (ValueError, TypeError):
                    result[key] = default_value
            return result
    except Exception as e:
        print(f"è·å–æ€§èƒ½é…ç½®å¤±è´¥: {e}")
    return DEFAULT_PERFORMANCE_CONFIG

def cleanup_expired_cache():
    """æ¸…ç†æ‰€æœ‰è¿‡æœŸç¼“å­˜"""
    current_time = time.time()
    perf_config = get_performance_config()
    cache_expire_time = perf_config.get("cache_expire_time", 30)

    with cache_lock:
        expired_keys = [k for k, (_, t) in file_list_cache.items() if current_time - t > cache_expire_time]
        for k in expired_keys:
            del file_list_cache[k]
        return len(expired_keys)

app = Flask(__name__)
app.config["APP_VERSION"] = get_app_ver()
app.secret_key = "ca943f6db6dd34823d36ab08d8d6f65d"
app.config["SESSION_COOKIE_NAME"] = "QUARK_AUTO_SAVE_X_SESSION"
app.config["PERMANENT_SESSION_LIFETIME"] = timedelta(days=31)
app.json.ensure_ascii = False
app.json.sort_keys = False
app.jinja_env.variable_start_string = "[["
app.jinja_env.variable_end_string = "]]"

scheduler = BackgroundScheduler()
logging.basicConfig(
    level=logging.DEBUG if DEBUG else logging.INFO,
    format="[%(asctime)s][%(levelname)s] %(message)s",
    datefmt="%m-%d %H:%M:%S",
)
# è¿‡æ»¤werkzeugæ—¥å¿—è¾“å‡º
if not DEBUG:
    logging.getLogger("werkzeug").setLevel(logging.ERROR)


def gen_md5(string):
    md5 = hashlib.md5()
    md5.update(string.encode("utf-8"))
    return md5.hexdigest()


def get_login_token():
    username = config_data["webui"]["username"]
    password = config_data["webui"]["password"]
    return gen_md5(f"token{username}{password}+-*/")[8:24]


def is_login():
    login_token = get_login_token()
    if session.get("token") == login_token or request.args.get("token") == login_token:
        return True
    else:
        return False


# è®¾ç½®icon
@app.route("/favicon.ico")
def favicon():
    return send_from_directory(
        os.path.join(app.root_path, "static", "images"),
        "favicon.ico",
        mimetype="image/vnd.microsoft.icon",
    )


# ç™»å½•é¡µé¢
@app.route("/login", methods=["GET", "POST"])
def login():
    if request.method == "POST":
        username = config_data["webui"]["username"]
        password = config_data["webui"]["password"]
        input_username = request.form.get("username")
        input_password = request.form.get("password")
        
        # éªŒè¯ç”¨æˆ·åå’Œå¯†ç 
        if not input_username or not input_password:
            logging.info(">>> ç™»å½•å¤±è´¥ï¼šç”¨æˆ·åæˆ–å¯†ç ä¸ºç©º")
            return render_template("login.html", message="ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º")
        elif username != input_username:
            logging.info(f">>> ç™»å½•å¤±è´¥ï¼šç”¨æˆ·åé”™è¯¯ {input_username}")
            return render_template("login.html", message="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
        elif password != input_password:
            logging.info(f">>> ç”¨æˆ· {input_username} ç™»å½•å¤±è´¥ï¼šå¯†ç é”™è¯¯")
            return render_template("login.html", message="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
        else:
            logging.info(f">>> ç”¨æˆ· {username} ç™»å½•æˆåŠŸ")
            session.permanent = True
            session["token"] = get_login_token()
            return redirect(url_for("index"))

    if is_login():
        return redirect(url_for("index"))
    return render_template("login.html", error=None)


# é€€å‡ºç™»å½•
@app.route("/logout")
def logout():
    session.pop("token", None)
    return redirect(url_for("login"))


# ç®¡ç†é¡µé¢
@app.route("/")
def index():
    if not is_login():
        return redirect(url_for("login"))
    return render_template(
        "index.html", version=app.config["APP_VERSION"], plugin_flags=PLUGIN_FLAGS
    )


# è·å–é…ç½®æ•°æ®
@app.route("/data")
def get_data():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    data = Config.read_json(CONFIG_PATH)

    # å¤„ç†æ’ä»¶é…ç½®ä¸­çš„å¤šè´¦å·æ”¯æŒå­—æ®µï¼Œå°†æ•°ç»„æ ¼å¼è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ç”¨äºæ˜¾ç¤º
    if "plugins" in data:
        # å¤„ç†Plexçš„quark_root_path
        if "plex" in data["plugins"] and "quark_root_path" in data["plugins"]["plex"]:
            data["plugins"]["plex"]["quark_root_path"] = format_array_config_for_display(
                data["plugins"]["plex"]["quark_root_path"]
            )

        # å¤„ç†AListçš„storage_id
        if "alist" in data["plugins"] and "storage_id" in data["plugins"]["alist"]:
            data["plugins"]["alist"]["storage_id"] = format_array_config_for_display(
                data["plugins"]["alist"]["storage_id"]
            )

    # å‘é€webuiä¿¡æ¯ï¼Œä½†ä¸å‘é€å¯†ç åŸæ–‡
    data["webui"] = {
        "username": config_data["webui"]["username"],
        "password": config_data["webui"]["password"]
    }
    data["api_token"] = get_login_token()
    data["task_plugins_config_default"] = task_plugins_config_default
    return jsonify({"success": True, "data": data})


def sync_task_plugins_config():
    """åŒæ­¥æ›´æ–°æ‰€æœ‰ä»»åŠ¡çš„æ’ä»¶é…ç½®
    
    1. æ£€æŸ¥æ¯ä¸ªä»»åŠ¡çš„æ’ä»¶é…ç½®
    2. å¦‚æœæ’ä»¶é…ç½®ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    3. å¦‚æœæ’ä»¶é…ç½®å­˜åœ¨ä½†ç¼ºå°‘æ–°çš„é…ç½®é¡¹ï¼Œæ·»åŠ é»˜è®¤å€¼
    4. ä¿ç•™åŸæœ‰çš„è‡ªå®šä¹‰é…ç½®
    5. åªå¤„ç†å·²å¯ç”¨çš„æ’ä»¶ï¼ˆé€šè¿‡PLUGIN_FLAGSæ£€æŸ¥ï¼‰
    6. æ¸…ç†è¢«ç¦ç”¨æ’ä»¶çš„é…ç½®
    """
    global config_data, task_plugins_config_default
    
    # å¦‚æœæ²¡æœ‰ä»»åŠ¡åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
    if not config_data.get("tasklist"):
        return
        
    # è·å–ç¦ç”¨çš„æ’ä»¶åˆ—è¡¨
    disabled_plugins = set()
    if PLUGIN_FLAGS:
        disabled_plugins = {name.lstrip('-') for name in PLUGIN_FLAGS.split(',')}
        
    # éå†æ‰€æœ‰ä»»åŠ¡
    for task in config_data["tasklist"]:
        # ç¡®ä¿ä»»åŠ¡æœ‰additionå­—æ®µ
        if "addition" not in task:
            task["addition"] = {}
            
        # æ¸…ç†è¢«ç¦ç”¨æ’ä»¶çš„é…ç½®
        for plugin_name in list(task["addition"].keys()):
            if plugin_name in disabled_plugins:
                del task["addition"][plugin_name]
            
        # éå†æ‰€æœ‰æ’ä»¶çš„é»˜è®¤é…ç½®
        for plugin_name, default_config in task_plugins_config_default.items():
            # è·³è¿‡è¢«ç¦ç”¨çš„æ’ä»¶
            if plugin_name in disabled_plugins:
                continue
                
            # å¦‚æœä»»åŠ¡ä¸­æ²¡æœ‰è¯¥æ’ä»¶çš„é…ç½®ï¼Œæ·»åŠ é»˜è®¤é…ç½®
            if plugin_name not in task["addition"]:
                task["addition"][plugin_name] = default_config.copy()
            else:
                # å¦‚æœä»»åŠ¡ä¸­æœ‰è¯¥æ’ä»¶çš„é…ç½®ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„é…ç½®é¡¹
                current_config = task["addition"][plugin_name]
                # ç¡®ä¿current_configæ˜¯å­—å…¸ç±»å‹
                if not isinstance(current_config, dict):
                    # å¦‚æœä¸æ˜¯å­—å…¸ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
                    task["addition"][plugin_name] = default_config.copy()
                    continue
                    
                # éå†é»˜è®¤é…ç½®çš„æ¯ä¸ªé”®å€¼å¯¹
                for key, default_value in default_config.items():
                    if key not in current_config:
                        current_config[key] = default_value


def parse_comma_separated_config(value):
    """è§£æé€—å·åˆ†éš”çš„é…ç½®å­—ç¬¦ä¸²ä¸ºæ•°ç»„"""
    if isinstance(value, str) and value.strip():
        # åˆ†å‰²å­—ç¬¦ä¸²ï¼Œå»é™¤ç©ºç™½å­—ç¬¦
        items = [item.strip() for item in value.split(',') if item.strip()]
        # å¦‚æœåªæœ‰ä¸€ä¸ªé¡¹ç›®ï¼Œè¿”å›å­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰
        return items[0] if len(items) == 1 else items
    return value

def format_array_config_for_display(value):
    """å°†æ•°ç»„é…ç½®æ ¼å¼åŒ–ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ç”¨äºæ˜¾ç¤º"""
    if isinstance(value, list):
        return ', '.join(value)
    return value

# æ›´æ–°æ•°æ®
@app.route("/update", methods=["POST"])
def update():
    global config_data
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    dont_save_keys = ["task_plugins_config_default", "api_token"]
    for key, value in request.json.items():
        if key not in dont_save_keys:
            if key == "webui":
                # æ›´æ–°webuiå‡­æ®
                config_data["webui"]["username"] = value.get("username", config_data["webui"]["username"])
                config_data["webui"]["password"] = value.get("password", config_data["webui"]["password"])
            elif key == "plugins":
                # å¤„ç†æ’ä»¶é…ç½®ä¸­çš„å¤šè´¦å·æ”¯æŒå­—æ®µ
                if "plex" in value and "quark_root_path" in value["plex"]:
                    value["plex"]["quark_root_path"] = parse_comma_separated_config(
                        value["plex"]["quark_root_path"]
                    )

                if "alist" in value and "storage_id" in value["alist"]:
                    value["alist"]["storage_id"] = parse_comma_separated_config(
                        value["alist"]["storage_id"]
                    )

                config_data.update({key: value})
            else:
                config_data.update({key: value})
    
    # åŒæ­¥æ›´æ–°ä»»åŠ¡çš„æ’ä»¶é…ç½®
    sync_task_plugins_config()
    
    Config.write_json(CONFIG_PATH, config_data)
    # æ›´æ–°session tokenï¼Œç¡®ä¿å½“å‰ä¼šè¯åœ¨ç”¨æˆ·åå¯†ç æ›´æ”¹åä»ç„¶æœ‰æ•ˆ
    session["token"] = get_login_token()
    # é‡æ–°åŠ è½½ä»»åŠ¡
    if reload_tasks():
        logging.info(f">>> é…ç½®æ›´æ–°æˆåŠŸ")
        return jsonify({"success": True, "message": "é…ç½®æ›´æ–°æˆåŠŸ"})
    else:
        logging.info(f">>> é…ç½®æ›´æ–°å¤±è´¥")
        return jsonify({"success": False, "message": "é…ç½®æ›´æ–°å¤±è´¥"})


# å¤„ç†è¿è¡Œè„šæœ¬è¯·æ±‚
@app.route("/run_script_now", methods=["POST"])
def run_script_now():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    tasklist = request.json.get("tasklist", [])
    command = [PYTHON_PATH, "-u", SCRIPT_PATH, CONFIG_PATH]
    logging.info(
        f">>> æ‰‹åŠ¨è¿è¡Œä»»åŠ¡ [{tasklist[0].get('taskname') if len(tasklist)>0 else 'ALL'}] å¼€å§‹æ‰§è¡Œ..."
    )

    def generate_output():
        # è®¾ç½®ç¯å¢ƒå˜é‡
        process_env = os.environ.copy()
        process_env["PYTHONIOENCODING"] = "utf-8"
        if tasklist:
            process_env["TASKLIST"] = json.dumps(tasklist, ensure_ascii=False)
            # æ·»åŠ åŸå§‹ä»»åŠ¡ç´¢å¼•çš„ç¯å¢ƒå˜é‡
            if len(tasklist) == 1 and 'original_index' in request.json:
                process_env["ORIGINAL_TASK_INDEX"] = str(request.json['original_index'])
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            encoding="utf-8",
            errors="replace",
            bufsize=1,
            env=process_env,
        )
        try:
            for line in iter(process.stdout.readline, ""):
                logging.info(line.strip())
                yield f"data: {line}\n\n"
            yield "data: [DONE]\n\n"
        finally:
            process.stdout.close()
            process.wait()

    return Response(
        stream_with_context(generate_output()),
        content_type="text/event-stream;charset=utf-8",
    )


# åˆ·æ–°Plexåª’ä½“åº“
@app.route("/refresh_plex_library", methods=["POST"])
def refresh_plex_library():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    
    task_index = request.json.get("task_index")
    if task_index is None:
        return jsonify({"success": False, "message": "ç¼ºå°‘ä»»åŠ¡ç´¢å¼•"})
        
    # è·å–ä»»åŠ¡ä¿¡æ¯
    task = config_data["tasklist"][task_index]
    if not task.get("savepath"):
        return jsonify({"success": False, "message": "ä»»åŠ¡æ²¡æœ‰ä¿å­˜è·¯å¾„"})
        
    # å¯¼å…¥Plexæ’ä»¶
    from plugins.plex import Plex
    
    # åˆå§‹åŒ–Plexæ’ä»¶
    plex = Plex(**config_data["plugins"]["plex"])
    if not plex.is_active:
        return jsonify({"success": False, "message": "Plex æ’ä»¶æœªæ­£ç¡®é…ç½®"})
        
    # æ‰§è¡Œåˆ·æ–°
    plex.run(task)
    
    return jsonify({"success": True, "message": "æˆåŠŸåˆ·æ–° Plex åª’ä½“åº“"})


# åˆ·æ–°AListç›®å½•
@app.route("/refresh_alist_directory", methods=["POST"])
def refresh_alist_directory():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    
    task_index = request.json.get("task_index")
    if task_index is None:
        return jsonify({"success": False, "message": "ç¼ºå°‘ä»»åŠ¡ç´¢å¼•"})
        
    # è·å–ä»»åŠ¡ä¿¡æ¯
    task = config_data["tasklist"][task_index]
    if not task.get("savepath"):
        return jsonify({"success": False, "message": "ä»»åŠ¡æ²¡æœ‰ä¿å­˜è·¯å¾„"})
        
    # å¯¼å…¥AListæ’ä»¶
    from plugins.alist import Alist
    
    # åˆå§‹åŒ–AListæ’ä»¶
    alist = Alist(**config_data["plugins"]["alist"])
    if not alist.is_active:
        return jsonify({"success": False, "message": "AList æ’ä»¶æœªæ­£ç¡®é…ç½®"})
        
    # æ‰§è¡Œåˆ·æ–°
    alist.run(task)
    
    return jsonify({"success": True, "message": "æˆåŠŸåˆ·æ–° AList ç›®å½•"})


# æ–‡ä»¶æ•´ç†é¡µé¢åˆ·æ–°Plexåª’ä½“åº“
@app.route("/refresh_filemanager_plex_library", methods=["POST"])
def refresh_filemanager_plex_library():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    folder_path = request.json.get("folder_path")
    account_index = request.json.get("account_index", 0)

    if not folder_path:
        return jsonify({"success": False, "message": "ç¼ºå°‘æ–‡ä»¶å¤¹è·¯å¾„"})

    # æ£€æŸ¥Plexæ’ä»¶é…ç½®
    if not config_data.get("plugins", {}).get("plex", {}).get("url"):
        return jsonify({"success": False, "message": "Plex æ’ä»¶æœªé…ç½®"})

    # å¯¼å…¥Plexæ’ä»¶
    from plugins.plex import Plex

    # åˆå§‹åŒ–Plexæ’ä»¶
    plex = Plex(**config_data["plugins"]["plex"])
    if not plex.is_active:
        return jsonify({"success": False, "message": "Plex æ’ä»¶æœªæ­£ç¡®é…ç½®"})

    # è·å–å¤¸å…‹è´¦å·ä¿¡æ¯
    try:
        account = Quark(config_data["cookie"][account_index], account_index)

        # å°†æ–‡ä»¶å¤¹è·¯å¾„è½¬æ¢ä¸ºå®é™…çš„ä¿å­˜è·¯å¾„
        # folder_pathæ˜¯ç›¸å¯¹äºå¤¸å…‹ç½‘ç›˜æ ¹ç›®å½•çš„è·¯å¾„
        # quark_root_pathæ˜¯å¤¸å…‹ç½‘ç›˜åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­çš„æŒ‚è½½ç‚¹
        # æ ¹æ®è´¦å·ç´¢å¼•è·å–å¯¹åº”çš„å¤¸å…‹æ ¹è·¯å¾„
        quark_root_path = plex.get_quark_root_path(account_index)
        if not quark_root_path:
            return jsonify({"success": False, "message": f"Plex æ’ä»¶æœªé…ç½®è´¦å· {account_index} çš„å¤¸å…‹æ ¹è·¯å¾„"})

        if folder_path == "" or folder_path == "/":
            # ç©ºå­—ç¬¦ä¸²æˆ–æ ¹ç›®å½•è¡¨ç¤ºå¤¸å…‹ç½‘ç›˜æ ¹ç›®å½•
            full_path = quark_root_path
        else:
            # ç¡®ä¿è·¯å¾„æ ¼å¼æ­£ç¡®
            if not folder_path.startswith("/"):
                folder_path = "/" + folder_path

            # æ‹¼æ¥å®Œæ•´è·¯å¾„ï¼šå¤¸å…‹æ ¹è·¯å¾„ + ç›¸å¯¹è·¯å¾„
            import os
            full_path = os.path.normpath(os.path.join(quark_root_path, folder_path.lstrip("/"))).replace("\\", "/")

        # ç¡®ä¿åº“ä¿¡æ¯å·²åŠ è½½
        if plex._libraries is None:
            plex._libraries = plex._get_libraries()

        # æ‰§è¡Œåˆ·æ–°
        success = plex.refresh(full_path)

        if success:
            return jsonify({"success": True, "message": "æˆåŠŸåˆ·æ–° Plex åª’ä½“åº“"})
        else:
            return jsonify({"success": False, "message": "åˆ·æ–° Plex åª’ä½“åº“å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„é…ç½®"})

    except Exception as e:
        return jsonify({"success": False, "message": f"åˆ·æ–° Plex åª’ä½“åº“å¤±è´¥: {str(e)}"})


# æ–‡ä»¶æ•´ç†é¡µé¢åˆ·æ–°AListç›®å½•
@app.route("/refresh_filemanager_alist_directory", methods=["POST"])
def refresh_filemanager_alist_directory():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    folder_path = request.json.get("folder_path")
    account_index = request.json.get("account_index", 0)

    if not folder_path:
        return jsonify({"success": False, "message": "ç¼ºå°‘æ–‡ä»¶å¤¹è·¯å¾„"})

    # æ£€æŸ¥AListæ’ä»¶é…ç½®
    if not config_data.get("plugins", {}).get("alist", {}).get("url"):
        return jsonify({"success": False, "message": "AList æ’ä»¶æœªé…ç½®"})

    # å¯¼å…¥AListæ’ä»¶
    from plugins.alist import Alist

    # åˆå§‹åŒ–AListæ’ä»¶
    alist = Alist(**config_data["plugins"]["alist"])
    if not alist.is_active:
        return jsonify({"success": False, "message": "AList æ’ä»¶æœªæ­£ç¡®é…ç½®"})

    # è·å–å¤¸å…‹è´¦å·ä¿¡æ¯
    try:
        account = Quark(config_data["cookie"][account_index], account_index)

        # å°†æ–‡ä»¶å¤¹è·¯å¾„è½¬æ¢ä¸ºå®é™…çš„ä¿å­˜è·¯å¾„
        # folder_pathæ˜¯ç›¸å¯¹äºå¤¸å…‹ç½‘ç›˜æ ¹ç›®å½•çš„è·¯å¾„ï¼Œå¦‚ "/" æˆ– "/æµ‹è¯•/æ–‡ä»¶å¤¹"
        # æ ¹æ®è´¦å·ç´¢å¼•è·å–å¯¹åº”çš„å­˜å‚¨é…ç½®
        storage_mount_path, quark_root_dir = alist.get_storage_config(account_index)

        if not storage_mount_path or not quark_root_dir:
            return jsonify({"success": False, "message": f"AList æ’ä»¶æœªé…ç½®è´¦å· {account_index} çš„å­˜å‚¨ä¿¡æ¯"})

        if folder_path == "/":
            # æ ¹ç›®å½•ï¼Œç›´æ¥ä½¿ç”¨å¤¸å…‹æ ¹è·¯å¾„
            full_path = quark_root_dir
        else:
            # å­ç›®å½•ï¼Œæ‹¼æ¥è·¯å¾„
            import os
            # ç§»é™¤folder_pathå¼€å¤´çš„/ï¼Œç„¶åæ‹¼æ¥
            relative_path = folder_path.lstrip("/")
            if quark_root_dir == "/":
                full_path = "/" + relative_path
            else:
                full_path = os.path.normpath(os.path.join(quark_root_dir, relative_path)).replace("\\", "/")

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨å¤¸å…‹æ ¹ç›®å½•å†…
        if quark_root_dir == "/" or full_path.startswith(quark_root_dir):
            # ä½¿ç”¨è´¦å·å¯¹åº”çš„å­˜å‚¨é…ç½®æ˜ å°„åˆ°AListè·¯å¾„
            # æ„å»ºAListè·¯å¾„
            if quark_root_dir == "/":
                relative_path = full_path.lstrip("/")
            else:
                relative_path = full_path.replace(quark_root_dir, "", 1).lstrip("/")

            alist_path = os.path.normpath(
                os.path.join(storage_mount_path, relative_path)
            ).replace("\\", "/")

            # æ‰§è¡Œåˆ·æ–°
            alist.refresh(alist_path)
            return jsonify({"success": True, "message": "æˆåŠŸåˆ·æ–° AList ç›®å½•"})
        else:
            return jsonify({"success": False, "message": "è·¯å¾„ä¸åœ¨AListé…ç½®çš„å¤¸å…‹æ ¹ç›®å½•å†…"})

    except Exception as e:
        return jsonify({"success": False, "message": f"åˆ·æ–° AList ç›®å½•å¤±è´¥: {str(e)}"})


@app.route("/task_suggestions")
def get_task_suggestions():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    query = request.args.get("q", "").lower()
    deep = request.args.get("d", "").lower()
    
    # æå–å‰§åï¼Œå»é™¤å­£æ•°ä¿¡æ¯
    def extract_show_name(task_name):
        # æ¸…ç†ä»»åŠ¡åç§°ä¸­çš„è¿ç»­ç©ºæ ¼å’Œç‰¹æ®Šç¬¦å·
        clean_name = task_name.replace('\u3000', ' ').replace('\t', ' ')
        clean_name = re.sub(r'\s+', ' ', clean_name).strip()
        
        # åŒ¹é…å¸¸è§çš„å­£æ•°æ ¼å¼
        # ä¾‹å¦‚ï¼šé»‘é•œ - S07ã€äººç”Ÿè‹¥å¦‚åˆè§ - S01ã€æŠ˜è…°.S01ã€éŸ³ä½ è€Œæ¥-S02ã€å¿«ä¹çš„å¤§äºº S02
        season_patterns = [
            r'^(.*?)[\s\.\-_]+S\d+$',  # é»‘é•œ - S07ã€æŠ˜è…°.S01ã€éŸ³ä½ è€Œæ¥-S02
            r'^(.*?)[\s\.\-_]+Season\s*\d+$',  # é»‘é•œ - Season 1
            r'^(.*?)\s+S\d+$',  # å¿«ä¹çš„å¤§äºº S02
            r'^(.*?)[\s\.\-_]+S\d+E\d+$',  # å¤„ç† S01E01 æ ¼å¼
            r'^(.*?)\s+ç¬¬\s*\d+\s*å­£$',  # å¤„ç† ç¬¬Nå­£ æ ¼å¼
            r'^(.*?)[\s\.\-_]+ç¬¬\s*\d+\s*å­£$',  # å¤„ç† - ç¬¬Nå­£ æ ¼å¼
            r'^(.*?)\s+ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶]+å­£$',  # å¤„ç† ç¬¬ä¸€å­£ã€ç¬¬äºŒå­£ æ ¼å¼
            r'^(.*?)[\s\.\-_]+ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åé›¶]+å­£$',  # å¤„ç† - ç¬¬ä¸€å­£ã€- ç¬¬äºŒå­£ æ ¼å¼
        ]
        
        for pattern in season_patterns:
            match = re.match(pattern, clean_name, re.IGNORECASE)
            if match:
                show_name = match.group(1).strip()
                # å»é™¤æœ«å°¾å¯èƒ½æ®‹ç•™çš„åˆ†éš”ç¬¦
                show_name = re.sub(r'[\s\.\-_]+$', '', show_name)
                return show_name
                
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å­£æ•°æ ¼å¼ï¼Œè¿”å›åŸåç§°
        return clean_name
    
    # å¤„ç†æœç´¢å…³é”®è¯ï¼Œæå–å‰§å
    search_query = extract_show_name(query)
    
    try:
        cs_data = config_data.get("source", {}).get("cloudsaver", {})
        if (
            cs_data.get("server")
            and cs_data.get("username")
            and cs_data.get("password")
        ):
            cs = CloudSaver(cs_data.get("server"))
            cs.set_auth(
                cs_data.get("username", ""),
                cs_data.get("password", ""),
                cs_data.get("token", ""),
            )
            # ä½¿ç”¨å¤„ç†åçš„æœç´¢å…³é”®è¯
            search = cs.auto_login_search(search_query)
            if search.get("success"):
                if search.get("new_token"):
                    cs_data["token"] = search.get("new_token")
                    Config.write_json(CONFIG_PATH, config_data)
                search_results = cs.clean_search_results(search.get("data"))
                # åœ¨è¿”å›ç»“æœä¸­æ·»åŠ å®é™…ä½¿ç”¨çš„æœç´¢å…³é”®è¯
                return jsonify(
                    {
                        "success": True, 
                        "source": "CloudSaver", 
                        "data": search_results
                    }
                )
            else:
                return jsonify({"success": True, "message": search.get("message")})
        else:
            base_url = base64.b64decode("aHR0cHM6Ly9zLjkxNzc4OC54eXo=").decode()
            # ä½¿ç”¨å¤„ç†åçš„æœç´¢å…³é”®è¯
            url = f"{base_url}/task_suggestions?q={search_query}&d={deep}"
            response = requests.get(url)
            return jsonify(
                {
                    "success": True, 
                    "source": "ç½‘ç»œå…¬å¼€", 
                    "data": response.json()
                }
            )
    except Exception as e:
        return jsonify({"success": True, "message": f"error: {str(e)}"})


# è·å–åˆ†äº«è¯¦æƒ…æ¥å£
@app.route("/get_share_detail", methods=["GET", "POST"])
def get_share_detail():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    
    # æ”¯æŒGETå’ŒPOSTè¯·æ±‚
    if request.method == "GET":
        shareurl = request.args.get("shareurl", "")
        stoken = request.args.get("stoken", "")
    else:
        shareurl = request.json.get("shareurl", "")
        stoken = request.json.get("stoken", "")
        
    account = Quark("", 0)
    # è®¾ç½®accountçš„å¿…è¦å±æ€§
    account.episode_patterns = request.json.get("regex", {}).get("episode_patterns", []) if request.method == "POST" else []
    
    pwd_id, passcode, pdir_fid, paths = account.extract_url(shareurl)
    if not stoken:
        is_sharing, stoken = account.get_stoken(pwd_id, passcode)
        if not is_sharing:
            return jsonify({"success": False, "data": {"error": stoken}})
    share_detail = account.get_detail(pwd_id, stoken, pdir_fid, _fetch_share=1)
    share_detail["paths"] = paths
    share_detail["stoken"] = stoken

    # å¤„ç†æ–‡ä»¶å¤¹çš„include_itemså­—æ®µï¼Œç¡®ä¿ç©ºæ–‡ä»¶å¤¹æ˜¾ç¤ºä¸º0é¡¹è€Œä¸æ˜¯undefined
    if "list" in share_detail and isinstance(share_detail["list"], list):
        for file_item in share_detail["list"]:
            if file_item.get("dir", False):
                # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œç¡®ä¿include_itemså­—æ®µå­˜åœ¨ä¸”ä¸ºæ•°å­—
                if "include_items" not in file_item or file_item["include_items"] is None:
                    file_item["include_items"] = 0
                elif not isinstance(file_item["include_items"], (int, float)):
                    # å¦‚æœinclude_itemsä¸æ˜¯æ•°å­—ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤±è´¥åˆ™è®¾ä¸º0
                    try:
                        file_item["include_items"] = int(file_item["include_items"])
                    except (ValueError, TypeError):
                        file_item["include_items"] = 0

    # å¦‚æœæ˜¯GETè¯·æ±‚æˆ–è€…ä¸éœ€è¦é¢„è§ˆæ­£åˆ™ï¼Œç›´æ¥è¿”å›åˆ†äº«è¯¦æƒ…
    if request.method == "GET" or not request.json.get("regex"):
        return jsonify({"success": True, "data": share_detail})
        
    # æ­£åˆ™å‘½åé¢„è§ˆ
    def preview_regex(share_detail):
        regex = request.json.get("regex")
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¡ºåºå‘½åæ¨¡å¼
        if regex.get("use_sequence_naming") and regex.get("sequence_naming"):
            # é¡ºåºå‘½åæ¨¡å¼é¢„è§ˆ
            sequence_pattern = regex.get("sequence_naming")
            current_sequence = 1
            
            # æ„å»ºé¡ºåºå‘½åçš„æ­£åˆ™è¡¨è¾¾å¼
            if sequence_pattern == "{}":
                # å¯¹äºå•ç‹¬çš„{}ï¼Œä½¿ç”¨ç‰¹æ®ŠåŒ¹é…
                regex_pattern = "(\\d+)"
            else:
                regex_pattern = re.escape(sequence_pattern).replace('\\{\\}', '(\\d+)')
            
            # å®ç°ä¸å®é™…é‡å‘½åç›¸åŒçš„æ’åºç®—æ³•
            def extract_sort_value(file):
                return sort_file_by_name(file)
            
            # è¿‡æ»¤å‡ºéç›®å½•æ–‡ä»¶ï¼Œå¹¶ä¸”æ’é™¤å·²ç»ç¬¦åˆå‘½åè§„åˆ™çš„æ–‡ä»¶
            files_to_process = []
            for f in share_detail["list"]:
                if f["dir"]:
                    continue  # è·³è¿‡ç›®å½•
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç¬¦åˆå‘½åè§„åˆ™
                if sequence_pattern == "{}":
                    # å¯¹äºå•ç‹¬çš„{}ï¼Œæ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä¸ºçº¯æ•°å­—
                    file_name_without_ext = os.path.splitext(f["file_name"])[0]
                    if file_name_without_ext.isdigit():
                        # å¢åŠ åˆ¤æ–­ï¼šå¦‚æœæ˜¯æ—¥æœŸæ ¼å¼çš„çº¯æ•°å­—ï¼Œä¸è§†ä¸ºå·²å‘½å
                        if not is_date_format(file_name_without_ext):
                            continue  # è·³è¿‡å·²ç¬¦åˆå‘½åè§„åˆ™çš„æ–‡ä»¶
                elif re.match(regex_pattern, f["file_name"]):
                    continue  # è·³è¿‡å·²ç¬¦åˆå‘½åè§„åˆ™çš„æ–‡ä»¶
                
                # æ·»åŠ åˆ°å¾…å¤„ç†æ–‡ä»¶åˆ—è¡¨
                files_to_process.append(f)
            
            # æ ¹æ®æå–çš„æ’åºå€¼è¿›è¡Œæ’åº
            sorted_files = sorted(files_to_process, key=extract_sort_value)
            
            # åº”ç”¨é«˜çº§è¿‡æ»¤è¯è¿‡æ»¤
            filterwords = regex.get("filterwords", "")
            if filterwords:
                # ä½¿ç”¨é«˜çº§è¿‡æ»¤å‡½æ•°
                filtered_files = advanced_filter_files(sorted_files, filterwords)
                # æ ‡è®°è¢«è¿‡æ»¤çš„æ–‡ä»¶
                for item in sorted_files:
                    if item not in filtered_files:
                        item["filtered"] = True
            
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ†é…åºå·
            for file in sorted_files:
                if not file.get("filtered"):
                    # è·å–æ–‡ä»¶æ‰©å±•å
                    file_ext = os.path.splitext(file["file_name"])[1]
                    # ç”Ÿæˆé¢„è§ˆæ–‡ä»¶å
                    if sequence_pattern == "{}":
                        # å¯¹äºå•ç‹¬çš„{}ï¼Œç›´æ¥ä½¿ç”¨æ•°å­—åºå·ä½œä¸ºæ–‡ä»¶å
                        file["file_name_re"] = f"{current_sequence:02d}{file_ext}"
                    else:
                        # æ›¿æ¢æ‰€æœ‰çš„{}ä¸ºå½“å‰åºå·
                        file["file_name_re"] = sequence_pattern.replace("{}", f"{current_sequence:02d}") + file_ext
                    current_sequence += 1
            
            return share_detail
        elif regex.get("use_episode_naming") and regex.get("episode_naming"):
            # å‰§é›†å‘½åæ¨¡å¼é¢„è§ˆ
            episode_pattern = regex.get("episode_naming")
            episode_patterns = regex.get("episode_patterns", [])
            
            # è·å–é»˜è®¤çš„å‰§é›†æ¨¡å¼
            default_episode_pattern = {"regex": 'ç¬¬(\\d+)é›†|ç¬¬(\\d+)æœŸ|ç¬¬(\\d+)è¯|(\\d+)é›†|(\\d+)æœŸ|(\\d+)è¯|[Ee][Pp]?(\\d+)|(\\d+)[-_\\s]*4[Kk]|\\[(\\d+)\\]|ã€(\\d+)ã€‘|_?(\\d+)_?'}
            
            # è·å–é…ç½®çš„å‰§é›†æ¨¡å¼ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡å¼éƒ½æ˜¯å­—å…¸æ ¼å¼
            episode_patterns = []
            raw_patterns = config_data.get("episode_patterns", [default_episode_pattern])
            for p in raw_patterns:
                if isinstance(p, dict) and p.get("regex"):
                    episode_patterns.append(p)
                elif isinstance(p, str):
                    episode_patterns.append({"regex": p})
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼
            if not episode_patterns:
                episode_patterns = [default_episode_pattern]
                
            # æ·»åŠ ä¸­æ–‡æ•°å­—åŒ¹é…æ¨¡å¼
            chinese_patterns = [
                {"regex": r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)é›†'},
                {"regex": r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)æœŸ'},
                {"regex": r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)è¯'},
                {"regex": r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)é›†'},
                {"regex": r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)æœŸ'},
                {"regex": r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)è¯'}
            ]
            episode_patterns.extend(chinese_patterns)
            
            # åº”ç”¨é«˜çº§è¿‡æ»¤è¯è¿‡æ»¤
            filterwords = regex.get("filterwords", "")
            if filterwords:
                # ä½¿ç”¨é«˜çº§è¿‡æ»¤å‡½æ•°
                filtered_files = advanced_filter_files(share_detail["list"], filterwords)
                # æ ‡è®°è¢«è¿‡æ»¤çš„æ–‡ä»¶
                for item in share_detail["list"]:
                    if item not in filtered_files:
                        item["filtered"] = True
                        item["file_name_re"] = "Ã—"
            
            # å¤„ç†æœªè¢«è¿‡æ»¤çš„æ–‡ä»¶
            for file in share_detail["list"]:
                if not file["dir"] and not file.get("filtered"):  # åªå¤„ç†æœªè¢«è¿‡æ»¤çš„éç›®å½•æ–‡ä»¶
                    extension = os.path.splitext(file["file_name"])[1]
                    # ä»æ–‡ä»¶åä¸­æå–é›†å·
                    episode_num = extract_episode_number(file["file_name"], episode_patterns=episode_patterns)

                    if episode_num is not None:
                        file["file_name_re"] = episode_pattern.replace("[]", f"{episode_num:02d}") + extension
                        # æ·»åŠ episode_numberå­—æ®µç”¨äºå‰ç«¯æ’åº
                        file["episode_number"] = episode_num
                    else:
                        # æ²¡æœ‰æå–åˆ°é›†å·ï¼Œæ˜¾ç¤ºæ— æ³•è¯†åˆ«çš„æç¤º
                        file["file_name_re"] = "Ã— æ— æ³•è¯†åˆ«å‰§é›†ç¼–å·"
            
            return share_detail
        else:
            # æ™®é€šæ­£åˆ™å‘½åé¢„è§ˆ
            pattern, replace = account.magic_regex_func(
                regex.get("pattern", ""),
                regex.get("replace", ""),
                regex.get("taskname", ""),
                regex.get("magic_regex", {}),
            )
            
            # åº”ç”¨é«˜çº§è¿‡æ»¤è¯è¿‡æ»¤
            filterwords = regex.get("filterwords", "")
            if filterwords:
                # ä½¿ç”¨é«˜çº§è¿‡æ»¤å‡½æ•°
                filtered_files = advanced_filter_files(share_detail["list"], filterwords)
                # æ ‡è®°è¢«è¿‡æ»¤çš„æ–‡ä»¶
                for item in share_detail["list"]:
                    if item not in filtered_files:
                        item["filtered"] = True
                
            # åº”ç”¨æ­£åˆ™å‘½å
            for item in share_detail["list"]:
                # åªå¯¹æœªè¢«è¿‡æ»¤çš„æ–‡ä»¶åº”ç”¨æ­£åˆ™å‘½å
                if not item.get("filtered") and re.search(pattern, item["file_name"]):
                    file_name = item["file_name"]
                    item["file_name_re"] = (
                        re.sub(pattern, replace, file_name) if replace != "" else file_name
                    )
            return share_detail

    share_detail = preview_regex(share_detail)

    # å†æ¬¡å¤„ç†æ–‡ä»¶å¤¹çš„include_itemså­—æ®µï¼Œç¡®ä¿é¢„è§ˆåçš„æ•°æ®ä¹Ÿæ­£ç¡®
    if "list" in share_detail and isinstance(share_detail["list"], list):
        for file_item in share_detail["list"]:
            if file_item.get("dir", False):
                # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œç¡®ä¿include_itemså­—æ®µå­˜åœ¨ä¸”ä¸ºæ•°å­—
                if "include_items" not in file_item or file_item["include_items"] is None:
                    file_item["include_items"] = 0
                elif not isinstance(file_item["include_items"], (int, float)):
                    # å¦‚æœinclude_itemsä¸æ˜¯æ•°å­—ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤±è´¥åˆ™è®¾ä¸º0
                    try:
                        file_item["include_items"] = int(file_item["include_items"])
                    except (ValueError, TypeError):
                        file_item["include_items"] = 0

    return jsonify({"success": True, "data": share_detail})


@app.route("/get_savepath_detail")
def get_savepath_detail():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    # è·å–è´¦å·ç´¢å¼•å‚æ•°
    account_index = int(request.args.get("account_index", 0))

    # éªŒè¯è´¦å·ç´¢å¼•
    if account_index < 0 or account_index >= len(config_data["cookie"]):
        return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

    account = Quark(config_data["cookie"][account_index], account_index)
    paths = []
    if path := request.args.get("path"):
        if path == "/":
            fid = 0
        else:
            dir_names = path.split("/")
            if dir_names[0] == "":
                dir_names.pop(0)
            path_fids = []
            current_path = ""
            for dir_name in dir_names:
                current_path += "/" + dir_name
                path_fids.append(current_path)
            if get_fids := account.get_fids(path_fids):
                fid = get_fids[-1]["fid"]
                paths = [
                    {"fid": get_fid["fid"], "name": dir_name}
                    for get_fid, dir_name in zip(get_fids, dir_names)
                ]
            else:
                return jsonify({"success": False, "data": {"error": "è·å–fidå¤±è´¥"}})
    else:
        fid = request.args.get("fid", "0")

    # è·å–æ–‡ä»¶åˆ—è¡¨
    files = account.ls_dir(fid)

    # å¤„ç†æ–‡ä»¶å¤¹çš„include_itemså­—æ®µï¼Œç¡®ä¿ç©ºæ–‡ä»¶å¤¹æ˜¾ç¤ºä¸º0é¡¹è€Œä¸æ˜¯undefined
    if isinstance(files, list):
        for file_item in files:
            if file_item.get("dir", False):
                # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œç¡®ä¿include_itemså­—æ®µå­˜åœ¨ä¸”ä¸ºæ•°å­—
                if "include_items" not in file_item or file_item["include_items"] is None:
                    file_item["include_items"] = 0
                elif not isinstance(file_item["include_items"], (int, float)):
                    # å¦‚æœinclude_itemsä¸æ˜¯æ•°å­—ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤±è´¥åˆ™è®¾ä¸º0
                    try:
                        file_item["include_items"] = int(file_item["include_items"])
                    except (ValueError, TypeError):
                        file_item["include_items"] = 0

    file_list = {
        "list": files,
        "paths": paths,
    }
    return jsonify({"success": True, "data": file_list})


@app.route("/delete_file", methods=["POST"])
def delete_file():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    # è·å–è´¦å·ç´¢å¼•å‚æ•°
    account_index = int(request.json.get("account_index", 0))

    # éªŒè¯è´¦å·ç´¢å¼•
    if account_index < 0 or account_index >= len(config_data["cookie"]):
        return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

    account = Quark(config_data["cookie"][account_index], account_index)
    if fid := request.json.get("fid"):
        response = account.delete([fid])
        
        # å¤„ç†delete_recordså‚æ•°
        if request.json.get("delete_records") and response.get("code") == 0:
            try:
                # åˆå§‹åŒ–æ•°æ®åº“
                db = RecordDB()
                
                # è·å–save_pathå‚æ•°
                save_path = request.json.get("save_path", "")
                
                # å¦‚æœæ²¡æœ‰æä¾›save_pathï¼Œåˆ™ä¸åˆ é™¤ä»»ä½•è®°å½•
                if not save_path:
                    response["deleted_records"] = 0
                    # logging.info(f">>> åˆ é™¤æ–‡ä»¶ {fid} ä½†æœªæä¾›save_pathï¼Œä¸åˆ é™¤ä»»ä½•è®°å½•")
                    return jsonify(response)
                
                # æŸ¥è¯¢ä¸è¯¥æ–‡ä»¶IDå’Œsave_pathç›¸å…³çš„æ‰€æœ‰è®°å½•
                cursor = db.conn.cursor()
                
                # ä½¿ç”¨file_idå’Œsave_pathè¿›è¡Œç²¾ç¡®åŒ¹é…
                cursor.execute("SELECT id FROM transfer_records WHERE file_id = ? AND save_path = ?", (fid, save_path))
                record_ids = [row[0] for row in cursor.fetchall()]
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„file_idè®°å½•ï¼Œå°è¯•é€šè¿‡æ–‡ä»¶åæŸ¥æ‰¾
                if not record_ids:
                    # è·å–æ–‡ä»¶åï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    file_name = request.json.get("file_name", "")
                    if file_name:
                        # ä½¿ç”¨æ–‡ä»¶åå’Œsave_pathè¿›è¡Œç²¾ç¡®åŒ¹é…
                        cursor.execute("""
                            SELECT id FROM transfer_records 
                            WHERE (original_name = ? OR renamed_to = ?) 
                            AND save_path = ?
                        """, (file_name, file_name, save_path))
                        
                        record_ids = [row[0] for row in cursor.fetchall()]
                
                # åˆ é™¤æ‰¾åˆ°çš„æ‰€æœ‰è®°å½•
                deleted_count = 0
                for record_id in record_ids:
                    deleted_count += db.delete_record(record_id)
                
                # æ·»åŠ åˆ é™¤è®°å½•çš„ä¿¡æ¯åˆ°å“åº”ä¸­
                response["deleted_records"] = deleted_count
                # logging.info(f">>> åˆ é™¤æ–‡ä»¶ {fid} åŒæ—¶åˆ é™¤äº† {deleted_count} æ¡ç›¸å…³è®°å½•")
                
            except Exception as e:
                logging.error(f">>> åˆ é™¤è®°å½•æ—¶å‡ºé”™: {str(e)}")
                # ä¸å½±å“ä¸»æµç¨‹ï¼Œå³ä½¿åˆ é™¤è®°å½•å¤±è´¥ä¹Ÿè¿”å›æ–‡ä»¶åˆ é™¤æˆåŠŸ
    else:
        response = {"success": False, "message": "ç¼ºå¤±å¿…è¦å­—æ®µ: fid"}
    return jsonify(response)


@app.route("/move_file", methods=["POST"])
def move_file():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    # è·å–è´¦å·ç´¢å¼•å‚æ•°
    account_index = int(request.json.get("account_index", 0))

    # éªŒè¯è´¦å·ç´¢å¼•
    if account_index < 0 or account_index >= len(config_data["cookie"]):
        return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

    account = Quark(config_data["cookie"][account_index], account_index)

    # è·å–å‚æ•°
    file_ids = request.json.get("file_ids", [])
    target_folder_id = request.json.get("target_folder_id")

    if not file_ids:
        return jsonify({"success": False, "message": "ç¼ºå°‘æ–‡ä»¶IDåˆ—è¡¨"})

    if not target_folder_id:
        return jsonify({"success": False, "message": "ç¼ºå°‘ç›®æ ‡æ–‡ä»¶å¤¹ID"})

    try:
        # è°ƒç”¨å¤¸å…‹ç½‘ç›˜çš„ç§»åŠ¨API
        response = account.move(file_ids, target_folder_id)

        if response["code"] == 0:
            return jsonify({
                "success": True,
                "message": f"æˆåŠŸç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶",
                "moved_count": len(file_ids)
            })
        else:
            return jsonify({
                "success": False,
                "message": response.get("message", "ç§»åŠ¨å¤±è´¥")
            })
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"ç§»åŠ¨æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
        })


@app.route("/create_folder", methods=["POST"])
def create_folder():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    # è·å–è¯·æ±‚å‚æ•°
    data = request.json
    parent_folder_id = data.get("parent_folder_id")
    folder_name = data.get("folder_name", "æ–°å»ºæ–‡ä»¶å¤¹")
    account_index = int(data.get("account_index", 0))

    # éªŒè¯å‚æ•°
    if not parent_folder_id:
        return jsonify({"success": False, "message": "ç¼ºå°‘çˆ¶ç›®å½•ID"})

    # éªŒè¯è´¦å·ç´¢å¼•
    if account_index < 0 or account_index >= len(config_data["cookie"]):
        return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

    try:
        # åˆå§‹åŒ–å¤¸å…‹ç½‘ç›˜å®¢æˆ·ç«¯
        account = Quark(config_data["cookie"][account_index], account_index)

        # è°ƒç”¨æ–°å»ºæ–‡ä»¶å¤¹API
        response = account.mkdir_in_folder(parent_folder_id, folder_name)

        if response.get("code") == 0:
            # åˆ›å»ºæˆåŠŸï¼Œè¿”å›æ–°æ–‡ä»¶å¤¹ä¿¡æ¯
            new_folder = response.get("data", {})
            return jsonify({
                "success": True,
                "message": "æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ",
                "data": {
                    "fid": new_folder.get("fid"),
                    "file_name": new_folder.get("file_name", folder_name),
                    "dir": True,
                    "size": 0,
                    "updated_at": new_folder.get("updated_at"),
                    "include_items": 0
                }
            })
        else:
            # å¤„ç†ç‰¹å®šçš„é”™è¯¯ä¿¡æ¯
            error_message = response.get("message", "åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥")

            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒåæ–‡ä»¶å¤¹å†²çª
            if "åŒå" in error_message or "å·²å­˜åœ¨" in error_message or "é‡å¤" in error_message or "doloading" in error_message:
                error_message = "å·²å­˜åœ¨åŒåæ–‡ä»¶å¤¹ï¼Œè¯·ä¿®æ”¹åç§°åå†è¯•"

            return jsonify({
                "success": False,
                "message": error_message
            })

    except Exception as e:
        logging.error(f">>> åˆ›å»ºæ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}")
        return jsonify({
            "success": False,
            "message": f"åˆ›å»ºæ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}"
        })


# æ·»åŠ ä»»åŠ¡æ¥å£
@app.route("/api/add_task", methods=["POST"])
def add_task():
    global config_data
    # éªŒè¯token
    if not is_login():
        return jsonify({"success": False, "code": 1, "message": "æœªç™»å½•"}), 401
    # å¿…é€‰å­—æ®µ
    request_data = request.json
    required_fields = ["taskname", "shareurl", "savepath"]
    for field in required_fields:
        if field not in request_data or not request_data[field]:
            return (
                jsonify(
                    {"success": False, "code": 2, "message": f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}"}
                ),
                400,
            )
    # æ·»åŠ ä»»åŠ¡
    config_data["tasklist"].append(request_data)
    Config.write_json(CONFIG_PATH, config_data)
    logging.info(f">>> é€šè¿‡APIæ·»åŠ ä»»åŠ¡: {request_data['taskname']}")
    return jsonify(
        {"success": True, "code": 0, "message": "ä»»åŠ¡æ·»åŠ æˆåŠŸ", "data": request_data}
    )


# å®šæ—¶ä»»åŠ¡æ‰§è¡Œçš„å‡½æ•°
def run_python(args):
    logging.info(f">>> å®šæ—¶è¿è¡Œä»»åŠ¡")

    # åœ¨å®šæ—¶ä»»åŠ¡å¼€å§‹å‰æ¸…ç†è¿‡æœŸç¼“å­˜
    try:
        cleaned_count = cleanup_expired_cache()
        if cleaned_count > 0:
            logging.info(f">>> æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")
    except Exception as e:
        logging.warning(f">>> æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {e}")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦éšæœºå»¶è¿Ÿæ‰§è¡Œ
    if delay := config_data.get("crontab_delay"):
        try:
            delay_seconds = int(delay)
            if delay_seconds > 0:
                # åœ¨0åˆ°è®¾å®šå€¼ä¹‹é—´éšæœºé€‰æ‹©ä¸€ä¸ªå»¶è¿Ÿæ—¶é—´
                random_delay = random.randint(0, delay_seconds)
                logging.info(f">>> éšæœºå»¶è¿Ÿæ‰§è¡Œ {random_delay}ç§’")
                time.sleep(random_delay)
        except (ValueError, TypeError):
            logging.warning(f">>> å»¶è¿Ÿæ‰§è¡Œè®¾ç½®æ— æ•ˆ: {delay}")

    os.system(f"{PYTHON_PATH} {args}")


# é‡æ–°åŠ è½½ä»»åŠ¡
def reload_tasks():
    # è¯»å–å®šæ—¶è§„åˆ™
    if crontab := config_data.get("crontab"):
        if scheduler.state == 1:
            scheduler.pause()  # æš‚åœè°ƒåº¦å™¨
        trigger = CronTrigger.from_crontab(crontab)
        scheduler.remove_all_jobs()
        scheduler.add_job(
            run_python,
            trigger=trigger,
            args=[f"{SCRIPT_PATH} {CONFIG_PATH}"],
            id=SCRIPT_PATH,
        )
        if scheduler.state == 0:
            scheduler.start()
        elif scheduler.state == 2:
            scheduler.resume()
        scheduler_state_map = {0: "åœæ­¢", 1: "è¿è¡Œ", 2: "æš‚åœ"}
        logging.info(">>> é‡è½½è°ƒåº¦å™¨")
        logging.info(f"è°ƒåº¦çŠ¶æ€: {scheduler_state_map[scheduler.state]}")
        logging.info(f"å®šæ—¶è§„åˆ™: {crontab}")
        # è®°å½•å»¶è¿Ÿæ‰§è¡Œè®¾ç½®
        if delay := config_data.get("crontab_delay"):
            logging.info(f"å»¶è¿Ÿæ‰§è¡Œ: 0-{delay}ç§’")
        logging.info(f"ç°æœ‰ä»»åŠ¡: {scheduler.get_jobs()}")
        return True
    else:
        logging.info(">>> no crontab")
        return False


def init():
    global config_data, task_plugins_config_default
    logging.info(f">>> åˆå§‹åŒ–é…ç½®")
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CONFIG_PATH):
        if not os.path.exists(os.path.dirname(CONFIG_PATH)):
            os.makedirs(os.path.dirname(CONFIG_PATH))
        with open("quark_config.json", "rb") as src, open(CONFIG_PATH, "wb") as dest:
            dest.write(src.read())

    # è¯»å–é…ç½®
    config_data = Config.read_json(CONFIG_PATH)
    Config.breaking_change_update(config_data)

    # é»˜è®¤ç®¡ç†è´¦å·
    config_data["webui"] = {
        "username": os.environ.get("WEBUI_USERNAME")
        or config_data.get("webui", {}).get("username", "admin"),
        "password": os.environ.get("WEBUI_PASSWORD")
        or config_data.get("webui", {}).get("password", "admin"),
    }

    # é»˜è®¤å®šæ—¶è§„åˆ™
    if not config_data.get("crontab"):
        config_data["crontab"] = "0 8,18,20 * * *"
    
    # é»˜è®¤å»¶è¿Ÿæ‰§è¡Œè®¾ç½®
    if "crontab_delay" not in config_data:
        config_data["crontab_delay"] = 0

    # åˆå§‹åŒ–æ’ä»¶é…ç½®
    _, plugins_config_default, task_plugins_config_default = Config.load_plugins()
    plugins_config_default.update(config_data.get("plugins", {}))
    config_data["plugins"] = plugins_config_default
    
    # è·å–ç¦ç”¨çš„æ’ä»¶åˆ—è¡¨
    disabled_plugins = set()
    if PLUGIN_FLAGS:
        disabled_plugins = {name.lstrip('-') for name in PLUGIN_FLAGS.split(',')}
    
    # æ¸…ç†æ‰€æœ‰ä»»åŠ¡ä¸­è¢«ç¦ç”¨æ’ä»¶çš„é…ç½®
    if config_data.get("tasklist"):
        for task in config_data["tasklist"]:
            if "addition" in task:
                for plugin_name in list(task["addition"].keys()):
                    if plugin_name in disabled_plugins:
                        del task["addition"][plugin_name]
    
    # åŒæ­¥æ›´æ–°ä»»åŠ¡çš„æ’ä»¶é…ç½®
    sync_task_plugins_config()

    # æ›´æ–°é…ç½®
    Config.write_json(CONFIG_PATH, config_data)


# è·å–å†å²è½¬å­˜è®°å½•
@app.route("/history_records")
def get_history_records():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
        
    # è·å–è¯·æ±‚å‚æ•°
    page = int(request.args.get("page", 1))
    page_size = int(request.args.get("page_size", 20))
    sort_by = request.args.get("sort_by", "transfer_time")
    order = request.args.get("order", "desc")
    
    # è·å–ç­›é€‰å‚æ•°
    task_name_filter = request.args.get("task_name", "")
    keyword_filter = request.args.get("keyword", "")
    
    # æ˜¯å¦åªè¯·æ±‚æ‰€æœ‰ä»»åŠ¡åç§°
    get_all_task_names = request.args.get("get_all_task_names", "").lower() in ["true", "1", "yes"]
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = RecordDB()
    
    # å¦‚æœè¯·æ±‚æ‰€æœ‰ä»»åŠ¡åç§°ï¼Œå•ç‹¬æŸ¥è¯¢å¹¶è¿”å›
    if get_all_task_names:
        cursor = db.conn.cursor()
        cursor.execute("SELECT DISTINCT task_name FROM transfer_records WHERE task_name NOT IN ('rename', 'undo_rename') ORDER BY task_name")
        all_task_names = [row[0] for row in cursor.fetchall()]
        
        # å¦‚æœåŒæ—¶è¯·æ±‚åˆ†é¡µæ•°æ®ï¼Œç»§ç»­å¸¸è§„æŸ¥è¯¢
        if page > 0 and page_size > 0:
            result = db.get_records(
                page=page, 
                page_size=page_size, 
                sort_by=sort_by, 
                order=order,
                task_name_filter=task_name_filter,
                keyword_filter=keyword_filter,
                exclude_task_names=["rename", "undo_rename"]
            )
            # æ·»åŠ æ‰€æœ‰ä»»åŠ¡åç§°åˆ°ç»“æœä¸­
            result["all_task_names"] = all_task_names
            
            # å¤„ç†è®°å½•æ ¼å¼åŒ–
            format_records(result["records"])
            
            return jsonify({"success": True, "data": result})
        else:
            # åªè¿”å›ä»»åŠ¡åç§°
            return jsonify({"success": True, "data": {"all_task_names": all_task_names}})
    
    # å¸¸è§„æŸ¥è¯¢
    result = db.get_records(
        page=page, 
        page_size=page_size, 
        sort_by=sort_by, 
        order=order,
        task_name_filter=task_name_filter,
        keyword_filter=keyword_filter,
        exclude_task_names=["rename", "undo_rename"]
    )
    
    # å¤„ç†è®°å½•æ ¼å¼åŒ–
    format_records(result["records"])
    
    return jsonify({"success": True, "data": result})


# åˆ é™¤è½¬å­˜è®°å½•
@app.route("/delete_history_records", methods=["POST"])
def delete_history_records():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    
    # è·å–è¦åˆ é™¤çš„è®°å½•IDåˆ—è¡¨
    record_ids = request.json.get("record_ids", [])
    
    if not record_ids:
        return jsonify({"success": False, "message": "æœªæä¾›è¦åˆ é™¤çš„è®°å½•ID"})
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = RecordDB()
    
    # åˆ é™¤è®°å½•
    deleted_count = 0
    for record_id in record_ids:
        deleted_count += db.delete_record(record_id)
    
    return jsonify({
        "success": True,
        "message": f"æˆåŠŸåˆ é™¤ {deleted_count} æ¡è®°å½•",
        "deleted_count": deleted_count
    })


# è·å–ä»»åŠ¡æœ€æ–°è½¬å­˜ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ—¥æœŸå’Œæ–‡ä»¶ï¼‰
@app.route("/task_latest_info")
def get_task_latest_info():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    try:
        # åˆå§‹åŒ–æ•°æ®åº“
        db = RecordDB()
        cursor = db.conn.cursor()

        # è·å–æ‰€æœ‰ä»»åŠ¡çš„æœ€æ–°è½¬å­˜æ—¶é—´
        query = """
        SELECT task_name, MAX(transfer_time) as latest_transfer_time
        FROM transfer_records
        WHERE task_name NOT IN ('rename', 'undo_rename')
        GROUP BY task_name
        """
        cursor.execute(query)
        latest_times = cursor.fetchall()

        task_latest_records = {}  # å­˜å‚¨æœ€æ–°è½¬å­˜æ—¥æœŸ
        task_latest_files = {}    # å­˜å‚¨æœ€æ–°è½¬å­˜æ–‡ä»¶

        for task_name, latest_time in latest_times:
            if latest_time:
                # 1. å¤„ç†æœ€æ–°è½¬å­˜æ—¥æœŸ
                try:
                    # ç¡®ä¿æ—¶é—´æˆ³åœ¨åˆç†èŒƒå›´å†…
                    timestamp = int(latest_time)
                    if timestamp > 9999999999:  # æ£€æµ‹æ˜¯å¦ä¸ºæ¯«ç§’çº§æ—¶é—´æˆ³ï¼ˆ13ä½ï¼‰
                        timestamp = timestamp / 1000  # è½¬æ¢ä¸ºç§’çº§æ—¶é—´æˆ³

                    if 0 < timestamp < 4102444800:  # ä»1970å¹´åˆ°2100å¹´çš„åˆç†æ—¶é—´æˆ³èŒƒå›´
                        # æ ¼å¼åŒ–ä¸ºæœˆ-æ—¥æ ¼å¼ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰å’Œå®Œæ•´æ—¥æœŸï¼ˆç”¨äºä»Šæ—¥åˆ¤æ–­ï¼‰
                        date_obj = datetime.fromtimestamp(timestamp)
                        formatted_date = date_obj.strftime("%m-%d")
                        full_date = date_obj.strftime("%Y-%m-%d")
                        task_latest_records[task_name] = {
                            "display": formatted_date,  # æ˜¾ç¤ºç”¨çš„ MM-DD æ ¼å¼
                            "full": full_date          # æ¯”è¾ƒç”¨çš„ YYYY-MM-DD æ ¼å¼
                        }
                except (ValueError, TypeError, OverflowError):
                    pass  # å¿½ç•¥æ— æ•ˆçš„æ—¶é—´æˆ³

                # 2. å¤„ç†æœ€æ–°è½¬å­˜æ–‡ä»¶
                # è·å–è¯¥ä»»åŠ¡åœ¨æœ€æ–°è½¬å­˜æ—¶é—´é™„è¿‘ï¼ˆåŒä¸€åˆ†é’Ÿå†…ï¼‰çš„æ‰€æœ‰æ–‡ä»¶
                # è¿™æ ·å¯ä»¥å¤„ç†åŒæ—¶è½¬å­˜å¤šä¸ªæ–‡ä»¶ä½†æ—¶é—´æˆ³ç•¥æœ‰å·®å¼‚çš„æƒ…å†µ
                time_window = 60000  # 60ç§’çš„æ—¶é—´çª—å£ï¼ˆæ¯«ç§’ï¼‰
                query = """
                SELECT renamed_to, original_name, transfer_time, modify_date
                FROM transfer_records
                WHERE task_name = ? AND transfer_time >= ? AND transfer_time <= ?
                ORDER BY id DESC
                """
                cursor.execute(query, (task_name, latest_time - time_window, latest_time + time_window))
                files = cursor.fetchall()

                if files:
                    if len(files) == 1:
                        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
                        best_file = files[0][0]  # renamed_to
                    else:
                        # å¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨å…¨å±€æ’åºå‡½æ•°è¿›è¡Œæ’åº
                        file_list = []
                        for renamed_to, original_name, transfer_time, modify_date in files:
                            # æ„é€ æ–‡ä»¶ä¿¡æ¯å­—å…¸ï¼Œæ¨¡æ‹Ÿå…¨å±€æ’åºå‡½æ•°éœ€è¦çš„æ ¼å¼
                            file_info = {
                                'file_name': renamed_to,
                                'original_name': original_name,
                                'updated_at': transfer_time  # ä½¿ç”¨è½¬å­˜æ—¶é—´è€Œä¸æ˜¯æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                            }
                            file_list.append(file_info)

                        # ä½¿ç”¨å…¨å±€æ’åºå‡½æ•°è¿›è¡Œæ­£å‘æ’åºï¼Œæœ€åä¸€ä¸ªå°±æ˜¯æœ€æ–°çš„
                        try:
                            sorted_files = sorted(file_list, key=sort_file_by_name)
                            best_file = sorted_files[-1]['file_name']  # å–æ’åºåçš„æœ€åä¸€ä¸ªæ–‡ä»¶
                        except Exception as e:
                            # å¦‚æœæ’åºå¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºå¤‡é€‰
                            best_file = files[0][0]

                    # å»é™¤æ‰©å±•åå¹¶å¤„ç†å­£æ•°é›†æ•°ä¿¡æ¯
                    if best_file:
                        file_name_without_ext = os.path.splitext(best_file)[0]
                        processed_name = process_season_episode_info(file_name_without_ext, task_name)
                        task_latest_files[task_name] = processed_name

        db.close()

        return jsonify({
            "success": True,
            "data": {
                "latest_records": task_latest_records,
                "latest_files": task_latest_files
            }
        })

    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"è·å–ä»»åŠ¡æœ€æ–°ä¿¡æ¯å¤±è´¥: {str(e)}"
        })



# åˆ é™¤å•æ¡è½¬å­˜è®°å½•
@app.route("/delete_history_record", methods=["POST"])
def delete_history_record():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    
    # è·å–è¦åˆ é™¤çš„è®°å½•ID
    record_id = request.json.get("id")
    
    if not record_id:
        return jsonify({"success": False, "message": "æœªæä¾›è¦åˆ é™¤çš„è®°å½•ID"})
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = RecordDB()
    
    # åˆ é™¤è®°å½•
    deleted = db.delete_record(record_id)
    
    if deleted:
        return jsonify({
            "success": True, 
            "message": "æˆåŠŸåˆ é™¤ 1 æ¡è®°å½•",
        })
    else:
        return jsonify({
            "success": False, 
            "message": "è®°å½•åˆ é™¤å¤±è´¥ï¼Œå¯èƒ½è®°å½•ä¸å­˜åœ¨",
        })


# è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–è®°å½•
def format_records(records):
    for record in records:
        # æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºå¯è¯»å½¢å¼
        if "transfer_time" in record:
            try:
                # ç¡®ä¿æ—¶é—´æˆ³åœ¨åˆç†èŒƒå›´å†…
                timestamp = int(record["transfer_time"])
                if timestamp > 9999999999:  # æ£€æµ‹æ˜¯å¦ä¸ºæ¯«ç§’çº§æ—¶é—´æˆ³ï¼ˆ13ä½ï¼‰
                    timestamp = timestamp / 1000  # è½¬æ¢ä¸ºç§’çº§æ—¶é—´æˆ³
                
                if 0 < timestamp < 4102444800:  # ä»1970å¹´åˆ°2100å¹´çš„åˆç†æ—¶é—´æˆ³èŒƒå›´
                    record["transfer_time_readable"] = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
                else:
                    record["transfer_time_readable"] = "æ— æ•ˆæ—¥æœŸ"
            except (ValueError, TypeError, OverflowError):
                record["transfer_time_readable"] = "æ— æ•ˆæ—¥æœŸ"
                
        if "modify_date" in record:
            try:
                # ç¡®ä¿æ—¶é—´æˆ³åœ¨åˆç†èŒƒå›´å†…
                timestamp = int(record["modify_date"])
                if timestamp > 9999999999:  # æ£€æµ‹æ˜¯å¦ä¸ºæ¯«ç§’çº§æ—¶é—´æˆ³ï¼ˆ13ä½ï¼‰
                    timestamp = timestamp / 1000  # è½¬æ¢ä¸ºç§’çº§æ—¶é—´æˆ³
                    
                if 0 < timestamp < 4102444800:  # ä»1970å¹´åˆ°2100å¹´çš„åˆç†æ—¶é—´æˆ³èŒƒå›´
                    record["modify_date_readable"] = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
                else:
                    record["modify_date_readable"] = "æ— æ•ˆæ—¥æœŸ"
            except (ValueError, TypeError, OverflowError):
                record["modify_date_readable"] = "æ— æ•ˆæ—¥æœŸ"
                
        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        if "file_size" in record:
            try:
                record["file_size_readable"] = format_bytes(int(record["file_size"]))
            except (ValueError, TypeError):
                record["file_size_readable"] = "æœªçŸ¥å¤§å°"


@app.route("/get_user_info")
def get_user_info():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    user_info_list = []
    for idx, cookie in enumerate(config_data["cookie"]):
        account = Quark(cookie, idx)
        account_info = account.init()
        if account_info:
            user_info_list.append({
                "index": idx,
                "nickname": account_info["nickname"],
                "is_active": account.is_active
            })
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç§»åŠ¨ç«¯å‚æ•°
            has_mparam = bool(account.mparam)
            user_info_list.append({
                "index": idx,
                "nickname": "",
                "is_active": False,
                "has_mparam": has_mparam
            })

    return jsonify({"success": True, "data": user_info_list})


@app.route("/get_accounts_detail")
def get_accounts_detail():
    """è·å–æ‰€æœ‰è´¦å·çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ˜µç§°å’Œç©ºé—´ä½¿ç”¨æƒ…å†µ"""
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    accounts_detail = []
    for idx, cookie in enumerate(config_data["cookie"]):
        account = Quark(cookie, idx)
        account_info = account.init()

        # å¦‚æœæ— æ³•è·å–è´¦å·ä¿¡æ¯ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç§»åŠ¨ç«¯å‚æ•°
        if not account_info:
            has_mparam = bool(account.mparam)
            # å¦‚æœåªæœ‰ç§»åŠ¨ç«¯å‚æ•°ï¼Œè·³è¿‡æ­¤è´¦å·ï¼ˆä¸æ˜¾ç¤ºåœ¨æ–‡ä»¶æ•´ç†é¡µé¢çš„è´¦å·é€‰æ‹©æ ä¸­ï¼‰
            if has_mparam:
                continue
            else:
                # å¦‚æœæ—¢æ²¡æœ‰è´¦å·ä¿¡æ¯ä¹Ÿæ²¡æœ‰ç§»åŠ¨ç«¯å‚æ•°ï¼Œæ˜¾ç¤ºä¸ºæœªç™»å½•
                account_detail = {
                    "index": idx,
                    "nickname": "",
                    "is_active": False,
                    "used_space": 0,
                    "total_space": 0,
                    "usage_rate": 0,
                    "display_text": f"è´¦å·{idx + 1}ï¼ˆæœªç™»å½•ï¼‰"
                }
                accounts_detail.append(account_detail)
                continue

        # æˆåŠŸè·å–è´¦å·ä¿¡æ¯çš„æƒ…å†µ
        account_detail = {
            "index": idx,
            "nickname": account_info["nickname"],
            "is_active": account.is_active,
            "used_space": 0,
            "total_space": 0,
            "usage_rate": 0,
            "display_text": ""
        }

        # æ£€æŸ¥æ˜¯å¦æœ‰ç§»åŠ¨ç«¯å‚æ•°
        has_mparam = bool(account.mparam)
        
        if has_mparam:
            # åŒæ—¶æœ‰cookieå’Œç§»åŠ¨ç«¯å‚æ•°ï¼Œå°è¯•è·å–ç©ºé—´ä¿¡æ¯
            try:
                growth_info = account.get_growth_info()
                if growth_info:
                    total_capacity = growth_info.get("total_capacity", 0)
                    account_detail["total_space"] = total_capacity
                    # æ˜¾ç¤ºæ˜µç§°å’Œæ€»å®¹é‡
                    total_str = format_bytes(total_capacity)
                    account_detail["display_text"] = f"{account_info['nickname']} Â· {total_str}"
                else:
                    # è·å–ç©ºé—´ä¿¡æ¯å¤±è´¥ï¼Œåªæ˜¾ç¤ºæ˜µç§°
                    account_detail["display_text"] = account_info["nickname"]
            except Exception as e:
                logging.error(f"è·å–è´¦å· {idx} ç©ºé—´ä¿¡æ¯å¤±è´¥: {str(e)}")
                # è·å–ç©ºé—´ä¿¡æ¯å¤±è´¥ï¼Œåªæ˜¾ç¤ºæ˜µç§°
                account_detail["display_text"] = account_info["nickname"]
        else:
            # åªæœ‰cookieï¼Œæ²¡æœ‰ç§»åŠ¨ç«¯å‚æ•°ï¼Œåªæ˜¾ç¤ºæ˜µç§°
            account_detail["display_text"] = account_info["nickname"]

        accounts_detail.append(account_detail)

    return jsonify({"success": True, "data": accounts_detail})


# é‡ç½®æ–‡ä»¶å¤¹ï¼ˆåˆ é™¤æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶å’Œç›¸å…³è®°å½•ï¼‰
@app.route("/reset_folder", methods=["POST"])
def reset_folder():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    # è·å–è¯·æ±‚å‚æ•°
    save_path = request.json.get("save_path", "")
    account_index = int(request.json.get("account_index", 0))  # æ–°å¢è´¦å·ç´¢å¼•å‚æ•°

    if not save_path:
        return jsonify({"success": False, "message": "ä¿å­˜è·¯å¾„ä¸èƒ½ä¸ºç©º"})

    try:
        # éªŒè¯è´¦å·ç´¢å¼•
        if account_index < 0 or account_index >= len(config_data["cookie"]):
            return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

        # åˆå§‹åŒ–å¤¸å…‹ç½‘ç›˜å®¢æˆ·ç«¯
        account = Quark(config_data["cookie"][account_index], account_index)
        
        # 1. è·å–æ–‡ä»¶å¤¹ID
        # å…ˆæ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„æ–‡ä»¶å¤¹ID
        folder_fid = account.savepath_fid.get(save_path)
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜çš„IDï¼Œåˆ™å°è¯•åˆ›å»ºæ–‡ä»¶å¤¹ä»¥è·å–ID
        if not folder_fid:
            mkdir_result = account.mkdir(save_path)
            if mkdir_result.get("code") == 0:
                folder_fid = mkdir_result["data"]["fid"]
                account.savepath_fid[save_path] = folder_fid
            else:
                return jsonify({"success": False, "message": f"è·å–æ–‡ä»¶å¤¹IDå¤±è´¥: {mkdir_result.get('message', 'æœªçŸ¥é”™è¯¯')}"})
        
        # 2. è·å–æ–‡ä»¶å¤¹å†…çš„æ‰€æœ‰æ–‡ä»¶
        file_list = account.ls_dir(folder_fid)
        if isinstance(file_list, dict) and file_list.get("error"):
            return jsonify({"success": False, "message": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {file_list.get('error', 'æœªçŸ¥é”™è¯¯')}"})
        
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶ID
        file_ids = []
        for item in file_list:
            file_ids.append(item["fid"])
        
        # 3. åˆ é™¤æ‰€æœ‰æ–‡ä»¶
        deleted_files = 0
        if file_ids:
            delete_result = account.delete(file_ids)
            if delete_result.get("code") == 0:
                deleted_files = len(file_ids)
        
        # 4. åˆ é™¤ç›¸å…³çš„å†å²è®°å½•
        deleted_records = 0
        try:
            # åˆå§‹åŒ–æ•°æ®åº“
            db = RecordDB()
            
            # æŸ¥è¯¢ä¸è¯¥ä¿å­˜è·¯å¾„ç›¸å…³çš„æ‰€æœ‰è®°å½•
            cursor = db.conn.cursor()
            cursor.execute("SELECT id FROM transfer_records WHERE save_path = ?", (save_path,))
            record_ids = [row[0] for row in cursor.fetchall()]
            
            # åˆ é™¤æ‰¾åˆ°çš„æ‰€æœ‰è®°å½•
            for record_id in record_ids:
                deleted_records += db.delete_record(record_id)
                
        except Exception as e:
            logging.error(f">>> åˆ é™¤è®°å½•æ—¶å‡ºé”™: {str(e)}")
            # å³ä½¿åˆ é™¤è®°å½•å¤±è´¥ï¼Œä¹Ÿè¿”å›æ–‡ä»¶åˆ é™¤æˆåŠŸ
        
        return jsonify({
            "success": True, 
            "message": f"é‡ç½®æˆåŠŸï¼Œåˆ é™¤äº† {deleted_files} ä¸ªæ–‡ä»¶å’Œ {deleted_records} æ¡è®°å½•",
            "deleted_files": deleted_files,
            "deleted_records": deleted_records
        })
        
    except Exception as e:
        logging.error(f">>> é‡ç½®æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"success": False, "message": f"é‡ç½®æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}"})


# è·å–æ–‡ä»¶åˆ—è¡¨
@app.route("/file_list")
def get_file_list():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})

    # è·å–è¯·æ±‚å‚æ•°
    folder_id = request.args.get("folder_id", "root")
    sort_by = request.args.get("sort_by", "file_name")
    order = request.args.get("order", "asc")
    page = int(request.args.get("page", 1))
    page_size = int(request.args.get("page_size", 15))
    account_index = int(request.args.get("account_index", 0))
    force_refresh = request.args.get("force_refresh", "false").lower() == "true"  # æ–°å¢è´¦å·ç´¢å¼•å‚æ•°

    try:
        # éªŒè¯è´¦å·ç´¢å¼•
        if account_index < 0 or account_index >= len(config_data["cookie"]):
            return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

        # åˆå§‹åŒ–å¤¸å…‹ç½‘ç›˜å®¢æˆ·ç«¯
        account = Quark(config_data["cookie"][account_index], account_index)

        # è·å–æ–‡ä»¶åˆ—è¡¨
        if folder_id == "root":
            folder_id = "0"  # æ ¹ç›®å½•çš„IDä¸º0
            paths = []  # æ ¹ç›®å½•æ²¡æœ‰è·¯å¾„
        else:
            # è·å–å½“å‰æ–‡ä»¶å¤¹çš„è·¯å¾„
            paths = account.get_paths(folder_id)

        # è·å–æ€§èƒ½é…ç½®
        perf_config = get_performance_config()
        api_page_size = perf_config.get("api_page_size", 200)
        cache_expire_time = perf_config.get("cache_expire_time", 30)

        # ç¼“å­˜é”®
        cache_key = f"{account_index}_{folder_id}_{sort_by}_{order}"
        current_time = time.time()

        # æ£€æŸ¥ç¼“å­˜ï¼ˆé™¤éå¼ºåˆ¶åˆ·æ–°ï¼‰
        if not force_refresh:
            with cache_lock:
                if cache_key in file_list_cache:
                    cache_data, cache_time = file_list_cache[cache_key]
                    if current_time - cache_time < cache_expire_time:
                        # ä½¿ç”¨ç¼“å­˜æ•°æ®
                        cached_files = cache_data
                        total = len(cached_files)
                        start_idx = (page - 1) * page_size
                        end_idx = min(start_idx + page_size, total)
                        paginated_files = cached_files[start_idx:end_idx]

                        return jsonify({
                            "success": True,
                            "data": {
                                "list": paginated_files,
                                "total": total,
                                "paths": paths
                            }
                        })
        else:
            # å¼ºåˆ¶åˆ·æ–°æ—¶ï¼Œæ¸…é™¤å½“å‰ç›®å½•çš„ç¼“å­˜
            with cache_lock:
                if cache_key in file_list_cache:
                    del file_list_cache[cache_key]

        # æ— è®ºåˆ†é¡µè¿˜æ˜¯å…¨éƒ¨æ¨¡å¼ï¼Œéƒ½å¿…é¡»è·å–æ‰€æœ‰æ–‡ä»¶æ‰èƒ½è¿›è¡Œæ­£ç¡®çš„å…¨å±€æ’åº
        files = account.ls_dir(folder_id, page_size=api_page_size)

        if isinstance(files, dict) and files.get("error"):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•ä¸å­˜åœ¨çš„é”™è¯¯
            error_msg = files.get('error', 'æœªçŸ¥é”™è¯¯')
            if "ä¸å­˜åœ¨" in error_msg or "æ— æ•ˆ" in error_msg or "æ‰¾ä¸åˆ°" in error_msg:
                return jsonify({"success": False, "message": f"ç›®å½•ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®: {error_msg}"})
            else:
                return jsonify({"success": False, "message": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {error_msg}"})

        # å¤„ç†æ–‡ä»¶å¤¹çš„include_itemså­—æ®µï¼Œç¡®ä¿ç©ºæ–‡ä»¶å¤¹æ˜¾ç¤ºä¸º0é¡¹è€Œä¸æ˜¯undefined
        for file_item in files:
            if file_item.get("dir", False):
                # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œç¡®ä¿include_itemså­—æ®µå­˜åœ¨ä¸”ä¸ºæ•°å­—
                if "include_items" not in file_item or file_item["include_items"] is None:
                    file_item["include_items"] = 0
                elif not isinstance(file_item["include_items"], (int, float)):
                    # å¦‚æœinclude_itemsä¸æ˜¯æ•°å­—ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤±è´¥åˆ™è®¾ä¸º0
                    try:
                        file_item["include_items"] = int(file_item["include_items"])
                    except (ValueError, TypeError):
                        file_item["include_items"] = 0

        # è®¡ç®—æ€»æ•°
        total = len(files)

        # ä¼˜åŒ–æ’åºï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„æ’åºæ–¹æ³•
        def get_sort_key(file_item):
            if sort_by == "file_name":
                # ä½¿ç”¨æ‹¼éŸ³æ’åº
                return get_filename_pinyin_sort_key(file_item["file_name"])
            elif sort_by == "file_size":
                # æ–‡ä»¶å¤¹æŒ‰é¡¹ç›®æ•°é‡æ’åºï¼Œæ–‡ä»¶æŒ‰å¤§å°æ’åº
                return file_item.get("include_items", 0) if file_item["dir"] else file_item["size"]
            else:  # updated_at
                return file_item["updated_at"]

        # åˆ†ç¦»æ–‡ä»¶å¤¹å’Œæ–‡ä»¶ä»¥ä¼˜åŒ–æ’åº
        if sort_by == "updated_at":
            # ä¿®æ”¹æ—¥æœŸæ’åºæ—¶ä¸¥æ ¼æŒ‰ç…§æ—¥æœŸæ’åºï¼Œä¸åŒºåˆ†æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
            files.sort(key=get_sort_key, reverse=(order == "desc"))
            sorted_files = files
        else:
            # å…¶ä»–æ’åºæ—¶ç›®å½•å§‹ç»ˆåœ¨å‰é¢ï¼Œåˆ†åˆ«æ’åºä»¥æé«˜æ•ˆç‡
            directories = [f for f in files if f["dir"]]
            normal_files = [f for f in files if not f["dir"]]

            directories.sort(key=get_sort_key, reverse=(order == "desc"))
            normal_files.sort(key=get_sort_key, reverse=(order == "desc"))

            sorted_files = directories + normal_files

        # æ›´æ–°ç¼“å­˜
        with cache_lock:
            file_list_cache[cache_key] = (sorted_files, current_time)

        # åˆ†é¡µ
        start_idx = (page - 1) * page_size
        end_idx = min(start_idx + page_size, total)
        paginated_files = sorted_files[start_idx:end_idx]

        return jsonify({
            "success": True,
            "data": {
                "list": paginated_files,
                "total": total,
                "paths": paths
            }
        })
        
    except Exception as e:
        logging.error(f">>> è·å–æ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"success": False, "message": f"è·å–æ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}"})


# é¢„è§ˆé‡å‘½å
@app.route("/preview_rename")
def preview_rename():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    
    # è·å–è¯·æ±‚å‚æ•°
    folder_id = request.args.get("folder_id", "root")
    pattern = request.args.get("pattern", "")
    replace = request.args.get("replace", "")
    naming_mode = request.args.get("naming_mode", "regex")  # regex, sequence, episode
    include_folders = request.args.get("include_folders", "false") == "true"
    filterwords = request.args.get("filterwords", "")
    account_index = int(request.args.get("account_index", 0))  # æ–°å¢è´¦å·ç´¢å¼•å‚æ•°

    if not pattern:
        pattern = ".*"

    try:
        # éªŒè¯è´¦å·ç´¢å¼•
        if account_index < 0 or account_index >= len(config_data["cookie"]):
            return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

        # åˆå§‹åŒ–å¤¸å…‹ç½‘ç›˜å®¢æˆ·ç«¯
        account = Quark(config_data["cookie"][account_index], account_index)
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        if folder_id == "root":
            folder_id = "0"  # æ ¹ç›®å½•çš„IDä¸º0
        
        files = account.ls_dir(folder_id)
        if isinstance(files, dict) and files.get("error"):
            return jsonify({"success": False, "message": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {files.get('error', 'æœªçŸ¥é”™è¯¯')}"})
        
        # ä½¿ç”¨é«˜çº§è¿‡æ»¤å‡½æ•°è¿‡æ»¤æ–‡ä»¶
        filtered_files = advanced_filter_files(files, filterwords)
        
        # å¦‚æœä¸åŒ…å«æ–‡ä»¶å¤¹ï¼Œè¿›ä¸€æ­¥è¿‡æ»¤æ‰æ–‡ä»¶å¤¹
        if not include_folders:
            filtered_files = [file for file in filtered_files if not file["dir"]]
        
        # æŒ‰ä¸åŒå‘½åæ¨¡å¼å¤„ç†
        preview_results = []
        
        if naming_mode == "sequence":
            # é¡ºåºå‘½åæ¨¡å¼
            # æ’åºæ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶åæˆ–ä¿®æ”¹æ—¶é—´ï¼‰
            filtered_files.sort(key=lambda x: sort_file_by_name(x["file_name"]))
            
            sequence = 1
            for file in filtered_files:
                extension = os.path.splitext(file["file_name"])[1] if not file["dir"] else ""
                new_name = pattern.replace("{}", f"{sequence:02d}") + extension
                preview_results.append({
                    "original_name": file["file_name"],
                    "new_name": new_name,
                    "file_id": file["fid"]
                })
                sequence += 1
                
        elif naming_mode == "episode":
            # å‰§é›†å‘½åæ¨¡å¼
            # è·å–é»˜è®¤çš„å‰§é›†æ¨¡å¼
            default_episode_pattern = {"regex": 'ç¬¬(\\d+)é›†|ç¬¬(\\d+)æœŸ|ç¬¬(\\d+)è¯|(\\d+)é›†|(\\d+)æœŸ|(\\d+)è¯|[Ee][Pp]?(\\d+)|(\\d+)[-_\\s]*4[Kk]|\\[(\\d+)\\]|ã€(\\d+)ã€‘|_?(\\d+)_?'}
            
            # è·å–é…ç½®çš„å‰§é›†æ¨¡å¼ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡å¼éƒ½æ˜¯å­—å…¸æ ¼å¼
            episode_patterns = []
            raw_patterns = config_data.get("episode_patterns", [default_episode_pattern])
            for p in raw_patterns:
                if isinstance(p, dict) and p.get("regex"):
                    episode_patterns.append(p)
                elif isinstance(p, str):
                    episode_patterns.append({"regex": p})
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼
            if not episode_patterns:
                episode_patterns = [default_episode_pattern]
                
            # æ·»åŠ ä¸­æ–‡æ•°å­—åŒ¹é…æ¨¡å¼
            chinese_patterns = [
                {"regex": r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)é›†'},
                {"regex": r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)æœŸ'},
                {"regex": r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)è¯'},
                {"regex": r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)é›†'},
                {"regex": r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)æœŸ'},
                {"regex": r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ä¸¤]+)è¯'}
            ]
            episode_patterns.extend(chinese_patterns)
            
            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            for file in filtered_files:
                extension = os.path.splitext(file["file_name"])[1] if not file["dir"] else ""
                # ä»æ–‡ä»¶åä¸­æå–é›†å·
                episode_num = extract_episode_number(file["file_name"], episode_patterns=episode_patterns)
                
                if episode_num is not None:
                    new_name = pattern.replace("[]", f"{episode_num:02d}") + extension
                    preview_results.append({
                        "original_name": file["file_name"],
                        "new_name": new_name,
                        "file_id": file["fid"],
                        "episode_number": episode_num  # æ·»åŠ é›†æ•°å­—æ®µç”¨äºå‰ç«¯æ’åº
                    })
                else:
                    # æ²¡æœ‰æå–åˆ°é›†å·ï¼Œæ˜¾ç¤ºæ— æ³•è¯†åˆ«çš„æç¤º
                    preview_results.append({
                        "original_name": file["file_name"],
                        "new_name": "Ã— æ— æ³•è¯†åˆ«å‰§é›†ç¼–å·",
                        "file_id": file["fid"]
                    })
        
        else:
            # æ­£åˆ™å‘½åæ¨¡å¼
            for file in filtered_files:
                try:
                    # åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼
                    if replace:
                        new_name = re.sub(pattern, replace, file["file_name"])
                    else:
                        # å¦‚æœæ²¡æœ‰æä¾›æ›¿æ¢è¡¨è¾¾å¼ï¼Œåˆ™æ£€æŸ¥æ˜¯å¦åŒ¹é…
                        if re.search(pattern, file["file_name"]):
                            new_name = file["file_name"]  # åŒ¹é…ä½†ä¸æ›¿æ¢
                        else:
                            new_name = ""  # è¡¨ç¤ºä¸åŒ¹é…
                            
                    preview_results.append({
                        "original_name": file["file_name"],
                        "new_name": new_name if new_name != file["file_name"] else "",  # å¦‚æœæ²¡æœ‰æ”¹å˜ï¼Œè¿”å›ç©ºè¡¨ç¤ºä¸é‡å‘½å
                        "file_id": file["fid"]
                    })
                except Exception as e:
                    # æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯
                    preview_results.append({
                        "original_name": file["file_name"],
                        "new_name": "",  # è¡¨ç¤ºæ— æ³•é‡å‘½å
                        "file_id": file["fid"],
                        "error": str(e)
                    })
        
        return jsonify({
            "success": True, 
            "data": preview_results
        })
        
    except Exception as e:
        logging.error(f">>> é¢„è§ˆé‡å‘½åæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"success": False, "message": f"é¢„è§ˆé‡å‘½åæ—¶å‡ºé”™: {str(e)}"})


# æ‰¹é‡é‡å‘½å
@app.route("/batch_rename", methods=["POST"])
def batch_rename():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    
    # è·å–è¯·æ±‚å‚æ•°
    data = request.json
    files = data.get("files", [])
    account_index = int(data.get("account_index", 0))  # æ–°å¢è´¦å·ç´¢å¼•å‚æ•°

    if not files:
        return jsonify({"success": False, "message": "æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½å"})

    try:
        # éªŒè¯è´¦å·ç´¢å¼•
        if account_index < 0 or account_index >= len(config_data["cookie"]):
            return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

        # åˆå§‹åŒ–å¤¸å…‹ç½‘ç›˜å®¢æˆ·ç«¯
        account = Quark(config_data["cookie"][account_index], account_index)
        
        # æ‰¹é‡é‡å‘½å
        success_count = 0
        failed_files = []
        save_path = data.get("save_path", "")
        batch_time = int(time.time() * 1000)  # æ‰¹æ¬¡æ—¶é—´æˆ³ï¼Œç¡®ä¿åŒä¸€æ‰¹transfer_timeä¸€è‡´
        for file_item in files:
            file_id = file_item.get("file_id")
            new_name = file_item.get("new_name")
            original_name = file_item.get("old_name") or ""
            if file_id and new_name:
                try:
                    result = account.rename(file_id, new_name)
                    if result.get("code") == 0:
                        success_count += 1
                        # è®°å½•é‡å‘½å
                        record_db.add_record(
                            task_name="rename",
                            original_name=original_name,
                            renamed_to=new_name,
                            file_size=0,
                            modify_date=int(time.time()),
                            file_id=file_id,
                            file_type="file",
                            save_path=save_path,
                            transfer_time=batch_time
                        )
                    else:
                        failed_files.append({
                            "file_id": file_id,
                            "new_name": new_name,
                            "error": result.get("message", "æœªçŸ¥é”™è¯¯")
                        })
                except Exception as e:
                    failed_files.append({
                        "file_id": file_id,
                        "new_name": new_name,
                        "error": str(e)
                    })
        
        return jsonify({
            "success": True, 
            "message": f"æˆåŠŸé‡å‘½å {success_count} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥ {len(failed_files)} ä¸ª",
            "success_count": success_count,
            "failed_files": failed_files
        })
        
    except Exception as e:
        logging.error(f">>> æ‰¹é‡é‡å‘½åæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"success": False, "message": f"æ‰¹é‡é‡å‘½åæ—¶å‡ºé”™: {str(e)}"})


# æ’¤é”€é‡å‘½åæ¥å£
@app.route("/undo_rename", methods=["POST"])
def undo_rename():
    if not is_login():
        return jsonify({"success": False, "message": "æœªç™»å½•"})
    data = request.json
    save_path = data.get("save_path", "")
    account_index = int(data.get("account_index", 0))  # æ–°å¢è´¦å·ç´¢å¼•å‚æ•°

    if not save_path:
        return jsonify({"success": False, "message": "ç¼ºå°‘ç›®å½•å‚æ•°"})

    try:
        # éªŒè¯è´¦å·ç´¢å¼•
        if account_index < 0 or account_index >= len(config_data["cookie"]):
            return jsonify({"success": False, "message": "è´¦å·ç´¢å¼•æ— æ•ˆ"})

        account = Quark(config_data["cookie"][account_index], account_index)
        # æŸ¥è¯¢è¯¥ç›®å½•ä¸‹æœ€è¿‘ä¸€æ¬¡é‡å‘½åï¼ˆæŒ‰transfer_timeåˆ†ç»„ï¼Œtask_name=renameï¼‰
        records = record_db.get_records_by_save_path(save_path)
        rename_records = [r for r in records if r["task_name"] == "rename"]
        if not rename_records:
            return jsonify({"success": False, "message": "æ²¡æœ‰å¯æ’¤é”€çš„é‡å‘½åè®°å½•"})
        # æ‰¾åˆ°æœ€è¿‘ä¸€æ‰¹ï¼ˆtransfer_timeæœ€å¤§å€¼ï¼‰
        latest_time = max(r["transfer_time"] for r in rename_records)
        latest_batch = [r for r in rename_records if r["transfer_time"] == latest_time]
        success_count = 0
        failed_files = []
        deleted_ids = []
        for rec in latest_batch:
            file_id = rec["file_id"]
            old_name = rec["original_name"]
            try:
                result = account.rename(file_id, old_name)
                if result.get("code") == 0:
                    success_count += 1
                    deleted_ids.append(rec["id"])
                else:
                    failed_files.append({
                        "file_id": file_id,
                        "old_name": old_name,
                        "error": result.get("message", "æœªçŸ¥é”™è¯¯")
                    })
            except Exception as e:
                failed_files.append({
                    "file_id": file_id,
                    "old_name": old_name,
                    "error": str(e)
                })
        # æ’¤é”€æˆåŠŸçš„ï¼Œç›´æ¥åˆ é™¤å¯¹åº”renameè®°å½•
        for rid in deleted_ids:
            record_db.delete_record(rid)
        return jsonify({
            "success": True,
            "message": f"æˆåŠŸæ’¤é”€ {success_count} ä¸ªæ–‡ä»¶é‡å‘½åï¼Œå¤±è´¥ {len(failed_files)} ä¸ª",
            "success_count": success_count,
            "failed_files": failed_files
        })
    except Exception as e:
        logging.error(f">>> æ’¤é”€é‡å‘½åæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"success": False, "message": f"æ’¤é”€é‡å‘½åæ—¶å‡ºé”™: {str(e)}"})


@app.route("/api/has_rename_record")
def has_rename_record():
    save_path = request.args.get("save_path", "")
    db = RecordDB()
    records = db.get_records_by_save_path(save_path)
    has_rename = any(r["task_name"] == "rename" for r in records)
    return jsonify({"has_rename": has_rename})


# è±†ç“£APIè·¯ç”±

# é€šç”¨ç”µå½±æ¥å£
@app.route("/api/douban/movie/recent_hot")
def get_movie_recent_hot():
    """è·å–ç”µå½±æ¦œå• - é€šç”¨æ¥å£"""
    try:
        category = request.args.get('category', 'çƒ­é—¨')
        type_param = request.args.get('type', 'å…¨éƒ¨')
        limit = int(request.args.get('limit', 20))
        start = int(request.args.get('start', 0))

        # æ˜ å°„categoryåˆ°main_category
        category_mapping = {
            'çƒ­é—¨': 'movie_hot',
            'æœ€æ–°': 'movie_latest',
            'è±†ç“£é«˜åˆ†': 'movie_top',
            'å†·é—¨ä½³ç‰‡': 'movie_underrated'
        }

        main_category = category_mapping.get(category, 'movie_hot')
        result = douban_service.get_list_data(main_category, type_param, limit, start)

        return jsonify(result)
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'è·å–ç”µå½±æ¦œå•å¤±è´¥: {str(e)}',
            'data': {'items': []}
        })

@app.route("/api/douban/movie/<movie_type>/<sub_category>")
def get_movie_list(movie_type, sub_category):
    """è·å–ç”µå½±æ¦œå•"""
    try:
        limit = int(request.args.get('limit', 20))
        start = int(request.args.get('start', 0))

        main_category = f"movie_{movie_type}"
        result = douban_service.get_list_data(main_category, sub_category, limit, start)

        return jsonify(result)
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'è·å–ç”µå½±æ¦œå•å¤±è´¥: {str(e)}',
            'data': {'items': []}
        })


# é€šç”¨ç”µè§†å‰§æ¥å£
@app.route("/api/douban/tv/recent_hot")
def get_tv_recent_hot():
    """è·å–ç”µè§†å‰§æ¦œå• - é€šç”¨æ¥å£"""
    try:
        category = request.args.get('category', 'tv')
        type_param = request.args.get('type', 'tv')
        limit = int(request.args.get('limit', 20))
        start = int(request.args.get('start', 0))

        # æ˜ å°„categoryåˆ°main_category
        if category == 'tv':
            main_category = 'tv_drama'
        elif category == 'show':
            main_category = 'tv_variety'
        elif category == 'tv_drama':
            main_category = 'tv_drama'
        elif category == 'tv_variety':
            main_category = 'tv_variety'
        else:
            main_category = 'tv_drama'

        result = douban_service.get_list_data(main_category, type_param, limit, start)

        return jsonify(result)
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'è·å–ç”µè§†å‰§æ¦œå•å¤±è´¥: {str(e)}',
            'data': {'items': []}
        })

@app.route("/api/douban/tv/<tv_type>/<sub_category>")
def get_tv_list(tv_type, sub_category):
    """è·å–ç”µè§†å‰§/ç»¼è‰ºæ¦œå•"""
    try:
        limit = int(request.args.get('limit', 20))
        start = int(request.args.get('start', 0))

        main_category = f"tv_{tv_type}"
        result = douban_service.get_list_data(main_category, sub_category, limit, start)

        return jsonify(result)
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'è·å–ç”µè§†å‰§æ¦œå•å¤±è´¥: {str(e)}',
            'data': {'items': []}
        })


if __name__ == "__main__":
    init()
    reload_tasks()
    # åˆå§‹åŒ–å…¨å±€dbå¯¹è±¡ï¼Œç¡®ä¿æ‰€æœ‰æ¥å£å¯ç”¨
    record_db = RecordDB()
    app.run(debug=DEBUG, host="0.0.0.0", port=PORT)
